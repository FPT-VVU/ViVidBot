{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 57 not upgraded.\n",
      "Need to get 175 kB of archives.\n",
      "After this operation, 386 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2 [175 kB]\n",
      "Fetched 175 kB in 1s (305 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 22469 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.2_amd64.deb ...\n",
      "Unpacking unzip (6.0-26ubuntu3.2) ...\n",
      "Setting up unzip (6.0-26ubuntu3.2) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsC85TuHVJ0L",
    "outputId": "1f82b8cb-b51f-48e8-b532-a12de0f4a350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.14.2.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting decord\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting prettytable\n",
      "  Downloading prettytable-3.10.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from deepspeed)\n",
      "  Downloading pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynvml (from deepspeed)\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting scipy>=1.9 (from scikit-image)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (10.0.1)\n",
      "Collecting imageio>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2024.5.22-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable) (0.2.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->deepspeed)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.3 (from pydantic->deepspeed)\n",
      "  Downloading pydantic_core-2.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading prettytable-3.10.0-py3-none-any.whl (28 kB)\n",
      "Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.5/225.5 kB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432247 sha256=17228c85f0c35427dda7ee15eb752f9777e0caf50e627b1371400a5a2148d2c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/7c/43/bed44d8414c099ff962b754f425f7ff77cc623cc8a98e0da70\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, xxhash, tzdata, tifffile, smmap, setproctitle, sentry-sdk, scipy, safetensors, regex, pynvml, pydantic-core, pyarrow-hotfix, pyarrow, protobuf, prettytable, opencv-python-headless, multidict, lazy-loader, imageio, frozenlist, einops, docker-pycreds, dill, decord, async-timeout, annotated-types, yarl, scikit-image, pydantic, pandas, multiprocess, huggingface-hub, gitdb, aiosignal, tokenizers, gitpython, deepspeed, aiohttp, accelerate, wandb, transformers, peft, datasets, evaluate\n",
      "Successfully installed accelerate-0.30.1 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 datasets-2.19.1 decord-0.6.0 deepspeed-0.14.2 dill-0.3.8 docker-pycreds-0.4.0 einops-0.8.0 evaluate-0.4.2 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 hjson-3.1.0 huggingface-hub-0.23.2 imageio-2.34.1 lazy-loader-0.4 multidict-6.0.5 multiprocess-0.70.16 ninja-1.11.1.1 opencv-python-headless-4.9.0.80 pandas-2.2.2 peft-0.11.1 prettytable-3.10.0 protobuf-4.25.3 py-cpuinfo-9.0.0 pyarrow-16.1.0 pyarrow-hotfix-0.6 pydantic-2.7.2 pydantic-core-2.18.3 pynvml-11.5.0 regex-2024.5.15 safetensors-0.4.3 scikit-image-0.23.2 scipy-1.13.1 sentry-sdk-2.3.1 setproctitle-1.3.3 smmap-5.0.1 tifffile-2024.5.22 tokenizers-0.19.1 transformers-4.41.1 tzdata-2024.1 wandb-0.17.0 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    " !pip install transformers accelerate deepspeed einops peft datasets evaluate decord scikit-image prettytable opencv-python-headless wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nIJj0hiCQQUE"
   },
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "DEFAULT_VIDEO_TOKEN = \"<video>\"\n",
    "DEFAULT_VIDEO_FRAME_TOKEN = \"<vi_frame>\"\n",
    "DEFAULT_VI_START_TOKEN = \"<vi_start>\"\n",
    "DEFAULT_VI_END_TOKEN = \"<vi_end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WKyHJ65_RMm2"
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "\n",
    "def _is_tensor_clip(clip):\n",
    "    return torch.is_tensor(clip) and clip.ndimension() == 4\n",
    "\n",
    "\n",
    "def crop_clip(clip, min_h, min_w, h, w):\n",
    "    if isinstance(clip[0], np.ndarray):\n",
    "        cropped = [img[min_h:min_h + h, min_w:min_w + w, :] for img in clip]\n",
    "\n",
    "    elif isinstance(clip[0], PIL.Image.Image):\n",
    "        cropped = [\n",
    "            img.crop((min_w, min_h, min_w + w, min_h + h)) for img in clip\n",
    "        ]\n",
    "    else:\n",
    "        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                        'but got list of {0}'.format(type(clip[0])))\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def resize_clip(clip, size, interpolation='bilinear'):\n",
    "    if isinstance(clip[0], np.ndarray):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "            # Min spatial dim already matches minimal size\n",
    "            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n",
    "                                                   and im_h == size):\n",
    "                return clip\n",
    "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
    "            size = (new_w, new_h)\n",
    "        else:\n",
    "            size = size[1], size[0]\n",
    "        if interpolation == 'bilinear':\n",
    "            np_inter = cv2.INTER_LINEAR\n",
    "        else:\n",
    "            np_inter = cv2.INTER_NEAREST\n",
    "        scaled = [\n",
    "            cv2.resize(img, size, interpolation=np_inter) for img in clip\n",
    "        ]\n",
    "    elif isinstance(clip[0], PIL.Image.Image):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            im_w, im_h = clip[0].size\n",
    "            # Min spatial dim already matches minimal size\n",
    "            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n",
    "                                                   and im_h == size):\n",
    "                return clip\n",
    "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
    "            size = (new_w, new_h)\n",
    "        else:\n",
    "            size = size[1], size[0]\n",
    "        if interpolation == 'bilinear':\n",
    "            pil_inter = PIL.Image.NEAREST\n",
    "        else:\n",
    "            pil_inter = PIL.Image.BILINEAR\n",
    "        scaled = [img.resize(size, pil_inter) for img in clip]\n",
    "    else:\n",
    "        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                        'but got list of {0}'.format(type(clip[0])))\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def get_resize_sizes(im_h, im_w, size):\n",
    "    if im_w < im_h:\n",
    "        ow = size\n",
    "        oh = int(size * im_h / im_w)\n",
    "    else:\n",
    "        oh = size\n",
    "        ow = int(size * im_w / im_h)\n",
    "    return oh, ow\n",
    "\n",
    "\n",
    "def normalize(clip, mean, std, inplace=False):\n",
    "    if not _is_tensor_clip(clip):\n",
    "        raise TypeError('tensor is not a torch clip_test.')\n",
    "\n",
    "    if not inplace:\n",
    "        clip = clip.clone()\n",
    "\n",
    "    dtype = clip.dtype\n",
    "    dim = len(mean)\n",
    "    mean = torch.as_tensor(mean, dtype=dtype, device=clip.device)\n",
    "    std = torch.as_tensor(std, dtype=dtype, device=clip.device)\n",
    "    # print(clip_test.size())\n",
    "    # if dim == 3:\n",
    "    clip.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\n",
    "    # else:\n",
    "    #     clip_test.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return clip\n",
    "\n",
    "\n",
    "def convert_img(img):\n",
    "    \"\"\"Converts (H, W, C) numpy.ndarray to (C, W, H) format\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = img.transpose(2, 0, 1)\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.expand_dims(img, 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "class ClipToTensor(object):\n",
    "    \"\"\"Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_nb=3, div_255=True, numpy=False):\n",
    "        self.channel_nb = channel_nb\n",
    "        self.div_255 = div_255\n",
    "        self.numpy = numpy\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args: clip_test (list of numpy.ndarray): clip_test (list of images)\n",
    "        to be converted to tensor.\n",
    "        \"\"\"\n",
    "        # Retrieve shape\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            h, w, ch = clip[0].shape\n",
    "            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(\n",
    "                ch)\n",
    "        elif isinstance(clip[0], Image.Image):\n",
    "            w, h = clip[0].size\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image\\\n",
    "            but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])\n",
    "\n",
    "        # Convert\n",
    "        for img_idx, img in enumerate(clip):\n",
    "            if isinstance(img, np.ndarray):\n",
    "                pass\n",
    "            elif isinstance(img, Image.Image):\n",
    "                img = np.array(img, copy=False)\n",
    "            else:\n",
    "                raise TypeError('Expected numpy.ndarray or PIL.Image\\\n",
    "                but got list of {0}'.format(type(clip[0])))\n",
    "            img = convert_img(img)\n",
    "            np_clip[:, img_idx, :, :] = img\n",
    "        if self.numpy:\n",
    "            if self.div_255:\n",
    "                np_clip = np_clip / 255\n",
    "            return np_clip\n",
    "\n",
    "        else:\n",
    "            tensor_clip = torch.from_numpy(np_clip)\n",
    "\n",
    "            if not isinstance(tensor_clip, torch.FloatTensor):\n",
    "                tensor_clip = tensor_clip.float()\n",
    "            if self.div_255:\n",
    "                tensor_clip = tensor_clip.div(255)\n",
    "            return tensor_clip\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Converts numpy array to tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, array):\n",
    "        tensor = torch.from_numpy(array)\n",
    "        return tensor\n",
    "\n",
    "class ColorDistortion(object):\n",
    "    def __init__(self, s=1.0):\n",
    "        self.s = s\n",
    "        self.color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "        self.rnd_color_jitter = transforms.RandomApply([self.color_jitter], p=0.8)\n",
    "        self.rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "\n",
    "    def __call__(self, video):\n",
    "        color_distort = transforms.Compose([self.rnd_color_jitter, self.rnd_gray])\n",
    "        return color_distort(video)\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms\n",
    "    Args:\n",
    "    transforms (list of ``Transform`` objects): list of transforms\n",
    "    to compose\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        for t in self.transforms:\n",
    "            clip = t(clip)\n",
    "        return clip\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the list of given images randomly\n",
    "    with a probability 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n",
    "        in format (h, w, c) in numpy.ndarray\n",
    "        Returns:\n",
    "        PIL.Image or numpy.ndarray: Randomly flipped clip_test\n",
    "        \"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            if isinstance(clip[0], np.ndarray):\n",
    "                return [np.fliplr(img) for img in clip]\n",
    "            elif isinstance(clip[0], PIL.Image.Image):\n",
    "                return [\n",
    "                    img.transpose(PIL.Image.FLIP_LEFT_RIGHT) for img in clip\n",
    "                ]\n",
    "            else:\n",
    "                raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                                ' but got list of {0}'.format(type(clip[0])))\n",
    "        return clip\n",
    "\n",
    "\n",
    "class RandomResize(object):\n",
    "    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n",
    "    The larger the original image is, the more times it takes to\n",
    "    interpolate\n",
    "    Args:\n",
    "    interpolation (str): Can be one of 'nearest', 'bilinear'\n",
    "    defaults to nearest\n",
    "    size (tuple): (widht, height)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio=(3. / 4., 4. / 3.), interpolation='nearest'):\n",
    "        self.ratio = ratio\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        scaling_factor = random.uniform(self.ratio[0], self.ratio[1])\n",
    "\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "\n",
    "        new_w = int(im_w * scaling_factor)\n",
    "        new_h = int(im_h * scaling_factor)\n",
    "        new_size = (new_w, new_h)\n",
    "        resized = resize_clip(\n",
    "            clip, new_size, interpolation=self.interpolation)\n",
    "        return resized\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n",
    "    The larger the original image is, the more times it takes to\n",
    "    interpolate\n",
    "    Args:\n",
    "    interpolation (str): Can be one of 'nearest', 'bilinear'\n",
    "    defaults to nearest\n",
    "    size (tuple): (widht, height)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation='nearest'):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        resized = resize_clip(\n",
    "            clip, self.size, interpolation=self.interpolation)\n",
    "        return resized\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Extract random crop at the same location for a list of images\n",
    "    Args:\n",
    "    size (sequence or int): Desired output size for the\n",
    "    crop in format (h, w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            size = (size, size)\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n",
    "        in format (h, w, c) in numpy.ndarray\n",
    "        Returns:\n",
    "        PIL.Image or numpy.ndarray: Cropped list of images\n",
    "        \"\"\"\n",
    "        h, w = self.size\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "        if w > im_w or h > im_h:\n",
    "            error_msg = (\n",
    "                'Initial image size should be larger then '\n",
    "                'cropped size but got cropped sizes : ({w}, {h}) while '\n",
    "                'initial image is ({im_w}, {im_h})'.format(\n",
    "                    im_w=im_w, im_h=im_h, w=w, h=h))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        x1 = random.randint(0, im_w - w)\n",
    "        y1 = random.randint(0, im_h - h)\n",
    "        cropped = crop_clip(clip, y1, x1, h, w)\n",
    "\n",
    "        return cropped\n",
    "\n",
    "\n",
    "class CornerCrop(object):\n",
    "\n",
    "    def __init__(self, size, crop_position=None):\n",
    "        self.size = size\n",
    "        if crop_position is None:\n",
    "            self.randomize = True\n",
    "        else:\n",
    "            self.randomize = False\n",
    "        self.crop_position = crop_position\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        t, h, w, c = imgs.shape\n",
    "        corner_imgs = list()\n",
    "        for n in self.crop_positions:\n",
    "            #print(n)\n",
    "            if n == 'c':\n",
    "                th, tw = (self.size, self.size)\n",
    "                x1 = int(round((w- tw) / 2.))\n",
    "                y1 = int(round((h - th) / 2.))\n",
    "                x2 = x1 + tw\n",
    "                y2 = y1 + th\n",
    "            elif n == 'tl':\n",
    "                x1 = 0\n",
    "                y1 = 0\n",
    "                x2 = self.size\n",
    "                y2 = self.size\n",
    "            elif n == 'tr':\n",
    "                x1 = w - self.size\n",
    "                y1 = 0\n",
    "                x2 = w\n",
    "                y2 = self.size\n",
    "            elif n == 'bl':\n",
    "                x1 = 0\n",
    "                y1 = h - self.size\n",
    "                x2 = self.size\n",
    "                y2 = h\n",
    "            elif n == 'br':\n",
    "                x1 = w - self.size\n",
    "                y1 = h - self.size\n",
    "                x2 = w\n",
    "                y2 = h\n",
    "            corner_imgs.append(imgs[:, y1:y2, x1:x2, :])\n",
    "        return corner_imgs\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        if self.randomize:\n",
    "            self.crop_position = self.crop_positions[random.randint(\n",
    "                0,\n",
    "                len(self.crop_positions) - 1)]\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    \"\"\"Rotate entire clip_test randomly by a random angle within\n",
    "    given bounds\n",
    "    Args:\n",
    "    degrees (sequence or int): Range of degrees to select from\n",
    "    If degrees is a number instead of sequence like (min, max),\n",
    "    the range of degrees, will be (-degrees, +degrees).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError('If degrees is a single number,'\n",
    "                                 'must be positive')\n",
    "            degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError('If degrees is a sequence,'\n",
    "                                 'it must be of len 2.')\n",
    "\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n",
    "        in format (h, w, c) in numpy.ndarray\n",
    "        Returns:\n",
    "        PIL.Image or numpy.ndarray: Cropped list of images\n",
    "        \"\"\"\n",
    "        angle = random.uniform(self.degrees[0], self.degrees[1])\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            rotated = [skimage.transform.rotate(img, angle) for img in clip]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            rotated = [img.rotate(angle) for img in clip]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        return rotated\n",
    "\n",
    "\n",
    "class STA_RandomRotation(object):\n",
    "    \"\"\"Rotate entire clip_test randomly by a random angle within\n",
    "    given bounds\n",
    "    Args:\n",
    "    degrees (sequence or int): Range of degrees to select from\n",
    "    If degrees is a number instead of sequence like (min, max),\n",
    "    the range of degrees, will be (-degrees, +degrees).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError('If degrees is a single number,'\n",
    "                                 'must be positive')\n",
    "            degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError('If degrees is a sequence,'\n",
    "                                 'it must be of len 2.')\n",
    "\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n",
    "        in format (h, w, c) in numpy.ndarray\n",
    "        Returns:\n",
    "        PIL.Image or numpy.ndarray: Cropped list of images\n",
    "        \"\"\"\n",
    "        bsz = len(clip)\n",
    "        angle = random.uniform(self.degrees[0], self.degrees[1])\n",
    "        angles = [(i+1)/(bsz+1) * angle for i in range(bsz)]\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            rotated = [skimage.transform.rotate(img, angles[i]) for i, img in enumerate(clip)]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            rotated = [img.rotate(angles[i]) for i, img in enumerate(clip)]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        return rotated\n",
    "\n",
    "\n",
    "class Each_RandomRotation(object):\n",
    "    \"\"\"Rotate entire clip_test randomly by a random angle within\n",
    "    given bounds\n",
    "    Args:\n",
    "    degrees (sequence or int): Range of degrees to select from\n",
    "    If degrees is a number instead of sequence like (min, max),\n",
    "    the range of degrees, will be (-degrees, +degrees).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError('If degrees is a single number,'\n",
    "                                 'must be positive')\n",
    "            degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError('If degrees is a sequence,'\n",
    "                                 'it must be of len 2.')\n",
    "\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n",
    "        in format (h, w, c) in numpy.ndarray\n",
    "        Returns:\n",
    "        PIL.Image or numpy.ndarray: Cropped list of images\n",
    "        \"\"\"\n",
    "        bsz = len(clip)\n",
    "        angles = [random.uniform(self.degrees[0], self.degrees[1]) for i in range(bsz)]\n",
    "        # print(angles)\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            rotated = [skimage.transform.rotate(img, angles[i]) for i, img in enumerate(clip)]\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            rotated = [img.rotate(angles[i]) for i, img in enumerate(clip)]\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "\n",
    "        return rotated\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Extract center crop at the same location for a list of images\n",
    "    Args:\n",
    "    size (sequence or int): Desired output size for the\n",
    "    crop in format (h, w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            size = (size, size)\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n",
    "        in format (h, w, c) in numpy.ndarray\n",
    "        Returns:\n",
    "        PIL.Image or numpy.ndarray: Cropped list of images\n",
    "        \"\"\"\n",
    "        h, w = self.size\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            im_h, im_w, im_c = clip[0].shape\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            im_w, im_h = clip[0].size\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "        if w > im_w or h > im_h:\n",
    "            error_msg = (\n",
    "                'Initial image size should be larger then '\n",
    "                'cropped size but got cropped sizes : ({w}, {h}) while '\n",
    "                'initial image is ({im_w}, {im_h})'.format(\n",
    "                    im_w=im_w, im_h=im_h, w=w, h=h))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        x1 = int(round((im_w - w) / 2.))\n",
    "        y1 = int(round((im_h - h) / 2.))\n",
    "        cropped = crop_clip(clip, y1, x1, h, w)\n",
    "\n",
    "        return cropped\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    \"\"\"Randomly change the brightness, contrast and saturation and hue of the clip_test\n",
    "    Args:\n",
    "    brightness (float): How much to jitter brightness. brightness_factor\n",
    "    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n",
    "    contrast (float): How much to jitter contrast. contrast_factor\n",
    "    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n",
    "    saturation (float): How much to jitter saturation. saturation_factor\n",
    "    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n",
    "    hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n",
    "    [-hue, hue]. Should be >=0 and <= 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "        self.hue = hue\n",
    "\n",
    "    def get_params(self, brightness, contrast, saturation, hue):\n",
    "        if brightness > 0:\n",
    "            brightness_factor = random.uniform(\n",
    "                max(0, 1 - brightness), 1 + brightness)\n",
    "        else:\n",
    "            brightness_factor = None\n",
    "\n",
    "        if contrast > 0:\n",
    "            contrast_factor = random.uniform(\n",
    "                max(0, 1 - contrast), 1 + contrast)\n",
    "        else:\n",
    "            contrast_factor = None\n",
    "\n",
    "        if saturation > 0:\n",
    "            saturation_factor = random.uniform(\n",
    "                max(0, 1 - saturation), 1 + saturation)\n",
    "        else:\n",
    "            saturation_factor = None\n",
    "\n",
    "        if hue > 0:\n",
    "            hue_factor = random.uniform(-hue, hue)\n",
    "        else:\n",
    "            hue_factor = None\n",
    "        return brightness_factor, contrast_factor, saturation_factor, hue_factor\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        clip_test (list): list of PIL.Image\n",
    "        Returns:\n",
    "        list PIL.Image : list of transformed PIL.Image\n",
    "        \"\"\"\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            raise TypeError(\n",
    "                'Color jitter not yet implemented for numpy arrays')\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            brightness, contrast, saturation, hue = self.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "\n",
    "            # Create img sync_dir function sequence\n",
    "            img_transforms = []\n",
    "            if brightness is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_brightness(img, brightness))\n",
    "            if saturation is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_saturation(img, saturation))\n",
    "            if hue is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_hue(img, hue))\n",
    "            if contrast is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_contrast(img, contrast))\n",
    "            random.shuffle(img_transforms)\n",
    "\n",
    "            # Apply to all images\n",
    "            jittered_clip = []\n",
    "            for img in clip:\n",
    "                for func in img_transforms:\n",
    "                    jittered_img = func(img)\n",
    "                jittered_clip.append(jittered_img)\n",
    "\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "        return jittered_clip\n",
    "\n",
    "\n",
    "class EachColorJitter(object):\n",
    "    \"\"\"Randomly change the brightness, contrast and saturation and hue of the clip_test\n",
    "    Args:\n",
    "    brightness (float): How much to jitter brightness. brightness_factor\n",
    "    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n",
    "    contrast (float): How much to jitter contrast. contrast_factor\n",
    "    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n",
    "    saturation (float): How much to jitter saturation. saturation_factor\n",
    "    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n",
    "    hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n",
    "    [-hue, hue]. Should be >=0 and <= 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "        self.hue = hue\n",
    "\n",
    "    def get_params(self, brightness, contrast, saturation, hue):\n",
    "        if brightness > 0:\n",
    "            brightness_factor = random.uniform(\n",
    "                max(0, 1 - brightness), 1 + brightness)\n",
    "        else:\n",
    "            brightness_factor = None\n",
    "\n",
    "        if contrast > 0:\n",
    "            contrast_factor = random.uniform(\n",
    "                max(0, 1 - contrast), 1 + contrast)\n",
    "        else:\n",
    "            contrast_factor = None\n",
    "\n",
    "        if saturation > 0:\n",
    "            saturation_factor = random.uniform(\n",
    "                max(0, 1 - saturation), 1 + saturation)\n",
    "        else:\n",
    "            saturation_factor = None\n",
    "\n",
    "        if hue > 0:\n",
    "            hue_factor = random.uniform(-hue, hue)\n",
    "        else:\n",
    "            hue_factor = None\n",
    "        return brightness_factor, contrast_factor, saturation_factor, hue_factor\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        clip_test (list): list of PIL.Image\n",
    "        Returns:\n",
    "        list PIL.Image : list of transformed PIL.Image\n",
    "        \"\"\"\n",
    "        if isinstance(clip[0], np.ndarray):\n",
    "            raise TypeError(\n",
    "                'Color jitter not yet implemented for numpy arrays')\n",
    "        elif isinstance(clip[0], PIL.Image.Image):\n",
    "            brightness, contrast, saturation, hue = self.get_params(\n",
    "                self.brightness, self.contrast, self.saturation, self.hue)\n",
    "\n",
    "            # Create img sync_dir function sequence\n",
    "            img_transforms = []\n",
    "            if brightness is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_brightness(img, brightness))\n",
    "            if saturation is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_saturation(img, saturation))\n",
    "            if hue is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_hue(img, hue))\n",
    "            if contrast is not None:\n",
    "                img_transforms.append(lambda img: torchvision.transforms.functional.adjust_contrast(img, contrast))\n",
    "            random.shuffle(img_transforms)\n",
    "\n",
    "            # Apply to all images\n",
    "            jittered_clip = []\n",
    "            for img in clip:\n",
    "                for func in img_transforms:\n",
    "                    jittered_img = func(img)\n",
    "                jittered_clip.append(jittered_img)\n",
    "\n",
    "        else:\n",
    "            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
    "                            'but got list of {0}'.format(type(clip[0])))\n",
    "        return jittered_clip\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize a clip_test with mean and standard deviation.\n",
    "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this sync_dir\n",
    "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
    "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "    .. note::\n",
    "        This sync_dir acts out of place, i.e., it does not mutates the input tensor.\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (Tensor): Tensor clip_test of size (T, C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor clip_test.\n",
    "        \"\"\"\n",
    "        return normalize(clip, self.mean, self.std)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "class TensorToNumpy(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        np_clip = clip.permute(1, 2, 3, 0).cpu().detach().numpy()\n",
    "        pil_clip = [Image.fromarray(np.uint8(numpy_image)).convert('RGB') for numpy_image in np_clip]\n",
    "        return pil_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5SVTk0CfSDpD"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Tuple\n",
    "import base64\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[str]]\n",
    "    offset: int\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: str = None\n",
    "    mode: str = None\n",
    "    skip_next: bool = False\n",
    "\n",
    "    def get_prompt(self):\n",
    "        if self.sep_style == SeparatorStyle.SINGLE:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = self.system + seps[0]\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + \": \" + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid style: {self.sep_style}\")\n",
    "\n",
    "    def append_message(self, role, message):\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "\n",
    "    def get_video(self,):\n",
    "        videos = []\n",
    "        path_list = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    msg, video_path, image_process_mode = msg\n",
    "                    path_list.append(video_path)\n",
    "                    with open(video_path, \"rb\") as videoFile:\n",
    "                        video_b64_str = base64.b64encode(videoFile.read())\n",
    "                    videos.append(video_b64_str)\n",
    "        return videos, path_list\n",
    "    def get_images(self, return_pil=False):\n",
    "        images = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    import base64\n",
    "                    from io import BytesIO\n",
    "                    from PIL import Image\n",
    "                    msg, image_list, image_process_mode = msg\n",
    "                    if type(image_list) is not list:\n",
    "                        image_list = [image_list]\n",
    "                    for image in image_list:\n",
    "                        if image_process_mode == \"Pad\":\n",
    "                            def expand2square(pil_img, background_color=(122, 116, 104)):\n",
    "                                width, height = pil_img.size\n",
    "                                if width == height:\n",
    "                                    return pil_img\n",
    "                                elif width > height:\n",
    "                                    result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                                    result.paste(pil_img, (0, (width - height) // 2))\n",
    "                                    return result\n",
    "                                else:\n",
    "                                    result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                                    result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                                    return result\n",
    "                            image = expand2square(image)\n",
    "                        elif image_process_mode == \"Crop\":\n",
    "                            pass\n",
    "                        elif image_process_mode == \"Resize\":\n",
    "                            image = image.resize((224, 224))\n",
    "                        else:\n",
    "                            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n",
    "                        max_hw, min_hw = max(image.size), min(image.size)\n",
    "                        aspect_ratio = max_hw / min_hw\n",
    "                        max_len, min_len = 800, 400\n",
    "                        shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n",
    "                        longest_edge = int(shortest_edge * aspect_ratio)\n",
    "                        W, H = image.size\n",
    "                        if H > W:\n",
    "                            H, W = longest_edge, shortest_edge\n",
    "                        else:\n",
    "                            H, W = shortest_edge, longest_edge\n",
    "                        image = image.resize((W, H))\n",
    "                        if return_pil:\n",
    "                            images.append(image)\n",
    "                        else:\n",
    "                            buffered = BytesIO()\n",
    "                            image.save(buffered, format=\"JPEG\")\n",
    "                            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "                            images.append(img_b64_str)\n",
    "        return images\n",
    "\n",
    "    def to_gradio_chatbot(self):\n",
    "        ret = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    import base64\n",
    "                    from io import BytesIO\n",
    "                    msg, image, image_process_mode = msg\n",
    "                    img_str = ''\n",
    "                    max_hw, min_hw = max(image.size), min(image.size)\n",
    "                    aspect_ratio = max_hw / min_hw\n",
    "                    max_len, min_len = 800, 400\n",
    "                    shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n",
    "                    longest_edge = int(shortest_edge * aspect_ratio)\n",
    "                    W, H = image.size\n",
    "                    if H > W:\n",
    "                        H, W = longest_edge, shortest_edge\n",
    "                    else:\n",
    "                        H, W = shortest_edge, longest_edge\n",
    "                    image = image.resize((W, H))\n",
    "                    # image = image.resize((224, 224))\n",
    "                    buffered = BytesIO()\n",
    "                    image.save(buffered, format=\"JPEG\")\n",
    "                    img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "                    img_str = img_str+f'<img src=\"data:image/png;base64,{img_b64_str}\" alt=\"user upload image\" />'\n",
    "                    msg = msg.replace('<image>', '')+img_str\n",
    "                    ret.append([msg, None])\n",
    "                else:\n",
    "                    ret.append([msg, None])\n",
    "            else:\n",
    "                ret[-1][-1] = msg\n",
    "        return ret\n",
    "\n",
    "    def video_to_gradio_chatbot(self):\n",
    "        ret = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "\n",
    "                    msg, video, image_process_mode = msg\n",
    "                    with open(video, \"rb\") as videoFile:\n",
    "                        video_b64_str = base64.b64encode(videoFile.read()).decode(\"utf-8\")\n",
    "                    img_str = ''\n",
    "                    img_str = img_str+f'''<video controls align=\"left\" style=\"height: 200px;\" src=\"data:video/mp4;base64,{video_b64_str}\">\n",
    "                                            The “video” tag is not supported by your browser. Click [here] to download the video file.\n",
    "                                            </video>'''\n",
    "                    msg = msg.replace('<video>', '')+img_str\n",
    "                    ret.append([msg, None])\n",
    "                else:\n",
    "                    ret.append([msg, None])\n",
    "            else:\n",
    "                ret[-1][-1] = msg\n",
    "        return ret\n",
    "\n",
    "    def copy(self):\n",
    "        return Conversation(\n",
    "            system=self.system,\n",
    "            roles=self.roles,\n",
    "            messages=[[x, y] for x, y in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2)\n",
    "\n",
    "    def dict(self):\n",
    "        if len(self.get_images()) > 0:\n",
    "            return {\n",
    "                \"system\": self.system,\n",
    "                \"roles\": self.roles,\n",
    "                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n",
    "                \"offset\": self.offset,\n",
    "                \"sep\": self.sep,\n",
    "                \"sep2\": self.sep2,\n",
    "            }\n",
    "        return {\n",
    "            \"system\": self.system,\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"offset\": self.offset,\n",
    "            \"sep\": self.sep,\n",
    "            \"sep2\": self.sep2,\n",
    "        }\n",
    "conv_v1_2 = Conversation(\n",
    "    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n",
    "    roles=(\"Human\", \"Assistant\"),\n",
    "    messages=(\n",
    "    ),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.SINGLE,\n",
    "    sep=\"###\",\n",
    ")\n",
    "\n",
    "simple_conv_video = Conversation(\n",
    "    system=\"You are Valley, a large language and vision assistant trained by ByteDance.\"\n",
    "           \"You are able to understand the visual content or video that the user provides, and assist the user with a variety of tasks using natural language.\"\n",
    "           \"Follow the instructions carefully and explain your answers in detail.\",\n",
    "    roles=(\"Human\", \"Assistant\"),\n",
    "    messages=(\n",
    "        (\"Human\", \"Hi!\"),\n",
    "        (\"Assistant\", \"Hi there!  How can I help you today?\\n\")\n",
    "    ),\n",
    "    offset=2,\n",
    "    sep_style=SeparatorStyle.SINGLE,\n",
    "    sep=\"###\",\n",
    ")\n",
    "default_conversation = simple_conv_video\n",
    "conv_templates = {\n",
    "    \"v1\":conv_v1_2,\n",
    "    \"multimodal_video\":simple_conv_video,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tj42w46MREe8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StoppingCriteria\n",
    "from typing import Dict, Sequence\n",
    "import transformers\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "import decord\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    image_list = [b[0] for b in batch]\n",
    "    prompt_list = [b[2] for b in batch]\n",
    "    # input_ids = pad_sequence(prompt_list, padding_value = 0, batch_first = True)\n",
    "    conv_list = [b[3] for b in batch]\n",
    "    label_list = [b[1] for b in batch]\n",
    "    return prompt_list, image_list, conv_list, label_list\n",
    "\n",
    "\n",
    "def collate_process_image_text(batch, tokenizer, image_processor):\n",
    "    batch_prompt, batch_image, conv_list, label_list = batch\n",
    "    batch_prompt = tokenizer(batch_prompt, padding=True)\n",
    "    input_ids, attention_mask = batch_prompt.input_ids, batch_prompt.attention_mask\n",
    "    input_ids = torch.as_tensor(input_ids)\n",
    "    attention_mask = torch.as_tensor(attention_mask)\n",
    "    videos = []\n",
    "    for this_batch_images in batch_image:\n",
    "        video = image_processor.preprocess(\n",
    "            this_batch_images, return_tensors='pt')['pixel_values']\n",
    "        videos.append(video)\n",
    "    return input_ids, attention_mask, videos, conv_list, label_list\n",
    "\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords, tokenizer, input_ids):\n",
    "        self.keywords = keywords\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_len = None\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if self.start_len is None:\n",
    "            self.start_len = self.input_ids.shape[1]\n",
    "        else:\n",
    "            outputs = self.tokenizer.batch_decode(\n",
    "                output_ids[:, self.start_len:], skip_special_tokens=True)[0]\n",
    "            for keyword in self.keywords:\n",
    "                if keyword in outputs:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "# for finetune\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "\n",
    "    if trainer.args.lora:\n",
    "        if trainer.args.should_save:\n",
    "            trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    else:\n",
    "        if trainer.deepspeed:\n",
    "            print('saving deepspeed model...')\n",
    "            torch.cuda.synchronize()\n",
    "            trainer.save_model(output_dir)\n",
    "            return\n",
    "\n",
    "        state_dict = trainer.model.state_dict()\n",
    "        if trainer.args.should_save:\n",
    "            cpu_state_dict = {\n",
    "                key: value.cpu()\n",
    "                for key, value in state_dict.items()\n",
    "            }\n",
    "            del state_dict\n",
    "            trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str],\n",
    "                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [\n",
    "        tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "    ]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def _mask_targets(target, tokenized_lens, speakers, only_mask_system):\n",
    "    # cur_idx = 0\n",
    "    cur_idx = tokenized_lens[0]\n",
    "    tokenized_lens = tokenized_lens[1:]\n",
    "    target[:cur_idx] = IGNORE_INDEX\n",
    "    if not only_mask_system:\n",
    "        for tokenized_len, speaker in zip(tokenized_lens, speakers):\n",
    "            if speaker == \"human\":\n",
    "                target[cur_idx+2:cur_idx + tokenized_len] = IGNORE_INDEX\n",
    "            cur_idx += tokenized_len\n",
    "\n",
    "\n",
    "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n",
    "    BEGIN_SIGNAL = \"### \"\n",
    "    END_SIGNAL = \"\\n\"\n",
    "    conversation = header\n",
    "    for sentence in source:\n",
    "        from_str = sentence[\"from\"]\n",
    "        if from_str.lower() == \"human\":\n",
    "            from_str = default_conversation.roles[0]\n",
    "        elif from_str.lower() == \"gpt\":\n",
    "            from_str = default_conversation.roles[1]\n",
    "        else:\n",
    "            from_str = 'unknown'\n",
    "        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n",
    "                             sentence[\"value\"] + END_SIGNAL)\n",
    "        if get_conversation:\n",
    "            conversation += sentence[\"value\"]\n",
    "    conversation += BEGIN_SIGNAL\n",
    "    return conversation\n",
    "\n",
    "\n",
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    multimodal_cfg: dict,\n",
    "    cur_token_len: int,\n",
    ") -> Dict:\n",
    "    is_multimodal = multimodal_cfg['is_multimodal']\n",
    "    # image_token_len = multimodal_cfg['image_token_len']\n",
    "    image_token_len = cur_token_len\n",
    "    if not is_multimodal:\n",
    "        return sources\n",
    "\n",
    "    for source in sources:\n",
    "        for sentence in source:\n",
    "            replace_token = DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\n",
    "            if multimodal_cfg['use_im_start_end']:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(\n",
    "                DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "    return sources\n",
    "\n",
    "\n",
    "def preprocess_multimodal_multiimage(\n",
    "    sources: Sequence[str],\n",
    "    multimodal_cfg: dict,\n",
    "    cur_token_len: int,\n",
    "    num_image: int\n",
    ") -> Dict:\n",
    "    is_multimodal = multimodal_cfg['is_multimodal']\n",
    "    # image_token_len = multimodal_cfg['image_token_len']\n",
    "    image_token_len = cur_token_len\n",
    "    if not is_multimodal:\n",
    "        return sources\n",
    "\n",
    "    for source in sources:\n",
    "        for sentence in source:\n",
    "            if multimodal_cfg['use_im_start_end']:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * \\\n",
    "                    image_token_len + DEFAULT_IM_END_TOKEN\n",
    "                replace_token = replace_token + DEFAULT_VI_START_TOKEN + \\\n",
    "                    DEFAULT_VIDEO_FRAME_TOKEN * num_image + DEFAULT_VI_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(\n",
    "                DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(\n",
    "                DEFAULT_VIDEO_TOKEN, replace_token)\n",
    "    return sources\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer, conv_mode, only_mask_system = False\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    for source in sources:\n",
    "        header = f\"{conv_templates[conv_mode].system}\\n\\n\"\n",
    "        conversation = _add_speaker_and_signal(header, source)\n",
    "        conversations.append(conversation)\n",
    "    # tokenize conversations\n",
    "    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
    "    input_ids = conversations_tokenized[\"input_ids\"]\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source],\n",
    "                                      tokenizer)[\"input_ids_lens\"]\n",
    "        speakers = [sentence[\"from\"] for sentence in source]\n",
    "        _mask_targets(target, tokenized_lens, speakers, only_mask_system)\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "\n",
    "def load_video(\n",
    "        path,\n",
    "        image_processer = None,\n",
    "        frame_mode='fixed',\n",
    "        fixed_frame_number=8,\n",
    "        fps_number=0.5,\n",
    "        frame_process_method='centercrop',\n",
    "):\n",
    "    if os.path.isfile(path):\n",
    "        video_reader = decord.VideoReader(\n",
    "            path, num_threads=1, ctx=decord.cpu(0))\n",
    "        decord.bridge.set_bridge('torch')\n",
    "        video_len = len(video_reader)\n",
    "\n",
    "        if frame_mode == 'fixed':\n",
    "            video = video_reader.get_batch(np.linspace(\n",
    "                0, video_len - 1, fixed_frame_number).astype(np.int_)).byte()  # 8, height,width,3\n",
    "            video = video.permute(3, 0, 1, 2)  # 3 x 8 x height x width\n",
    "        elif frame_mode == 'fps':\n",
    "            fps_offset = int(round(video_reader.get_avg_fps())/fps_number)\n",
    "            video = video_reader.get_batch(\n",
    "                range(0, video_len, fps_offset)).byte()\n",
    "            video = video.permute(3, 0, 1, 2)  # 3 x 8 x height x width\n",
    "        input_mean = [0.48145466, 0.4578275, 0.40821073] # Consistent with clilp preprocessing\n",
    "        input_std = [0.26862954, 0.26130258, 0.27577711] #Consistent with clilp preprocessing\n",
    "        crop_size, scale_size = 224, 256\n",
    "        trans = transforms.Compose([\n",
    "            TensorToNumpy(),\n",
    "            Resize(scale_size),\n",
    "            CenterCrop(crop_size),\n",
    "            ClipToTensor(channel_nb=3),\n",
    "            Normalize(mean=input_mean, std=input_std)\n",
    "        ])\n",
    "        video = trans(video)\n",
    "    else:\n",
    "        video_frames = list(Path(path).rglob('*'))\n",
    "        if frame_mode == 'fixed':\n",
    "            video_frames = [video_frames[i] for i in np.linspace(\n",
    "                0, len(video_frames) - 1, fixed_frame_number).astype(np.int_)]\n",
    "        elif frame_mode == 'fps':\n",
    "            raise ValueError('Input folder is not support this frame mode')\n",
    "        else:\n",
    "            raise ValueError('Frame mode is only support \"fps\" or \"fixed\"')\n",
    "        video_frames = [Image.open(str(path)) for path in video_frames]\n",
    "\n",
    "        if frame_process_method == 'resize':\n",
    "            min_length = min(video_frames[0].size)\n",
    "            resize = transforms.Resize([min_length, min_length])\n",
    "            video_frames = [resize(frame) for frame in video_frames]\n",
    "            # test_frame = video_frames[0]\n",
    "\n",
    "        video = image_processer.preprocess(\n",
    "            video_frames, return_tensors='pt')['pixel_values']\n",
    "        video = video.permute(1, 0, 2, 3)\n",
    "    return video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AbFEdl0vQMLK"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                        MptModel, MptConfig,  PretrainedConfig,  PreTrainedModel, MptForCausalLM, CLIPImageProcessor, CLIPVisionModel, PretrainedConfig\n",
    "\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "class ValleyConfig(MptConfig):\n",
    "    model_type = \"valley\"\n",
    "\n",
    "class ValleyGPTModel(MptModel):\n",
    "    config_class = ValleyConfig\n",
    "\n",
    "    def __init__(self, config: MptConfig, mm_vision_tower=None, mm_hidden_size=None):\n",
    "        super(ValleyGPTModel, self).__init__(config)\n",
    "\n",
    "        self.patch_pooling_method = \"mean\"\n",
    "\n",
    "        if hasattr(config, \"mm_vision_tower\"):\n",
    "            # HACK: for FSDP\n",
    "            # self.vision_tower = [CLIPVisionModel.from_pretrained(config.mm_vision_tower)]\n",
    "            if 'chinese' in config.mm_vision_tower:\n",
    "                from transformers import ChineseCLIPVisionModel as CLIPVisionModel\n",
    "                from transformers import ChineseCLIPImageProcessor as CLIPImageProcessor\n",
    "            else:\n",
    "                from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "            self.vision_tower = CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n",
    "\n",
    "        if hasattr(config, \"use_patch_importance_pooling\") and config.use_patch_importance_pooling:\n",
    "            print('using temporal linear pooling')\n",
    "            self.pooling_layer = nn.Linear(self.config.hidden_size * 256, 1)\n",
    "            self.patch_pooling_method = \"temporal_importance\"\n",
    "\n",
    "        if hasattr(config, \"use_delta_transformer\") and config.use_delta_transformer:\n",
    "            print('using temporal transformer delta adding')\n",
    "            self.transforemr_adding_layer = nn.TransformerEncoderLayer(d_model=config.hidden_size, nhead=8, batch_first=True)\n",
    "            self.transformer_delta_encoder = nn.TransformerEncoder(self.transforemr_adding_layer, num_layers=1)\n",
    "            self.patch_pooling_method = \"temporal_transformer\"\n",
    "            # self.position_matrix = torch.nn.Parameter(self.getPositionEncoding(seq_len=2048, d = config.hidden_size))\n",
    "            self.position_matrix = torch.nn.Parameter(torch.zeros(2048,config.hidden_size))\n",
    "            self.position_matrix.requires_grad = False\n",
    "\n",
    "        if hasattr(config, \"use_mm_proj\"):\n",
    "            self.mm_projector = nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "            print(config.mm_hidden_size, config.hidden_size)\n",
    "\n",
    "\n",
    "    def initialize_vision_modules(self, vision_tower, mm_vision_select_layer,\n",
    "                                  pretrain_mm_mlp_adapter=None, use_patch_importance_pooling=False, use_delta_transformer=False):\n",
    "        self.config.mm_vision_tower = vision_tower\n",
    "\n",
    "        image_processor = CLIPImageProcessor.from_pretrained(vision_tower)\n",
    "\n",
    "        if not hasattr(self, 'vision_tower'):\n",
    "            vision_tower = CLIPVisionModel.from_pretrained(vision_tower)\n",
    "        else:\n",
    "            vision_tower = self.vision_tower\n",
    "        vision_tower.requires_grad_(False)\n",
    "        # vision_tower = vision_tower.to(torch.float16)\n",
    "        self.vision_tower = vision_tower\n",
    "\n",
    "        vision_config = vision_tower.config\n",
    "        num_patches = (vision_config.image_size // vision_config.patch_size) ** 2\n",
    "\n",
    "        self.config.use_mm_proj = True\n",
    "        self.config.use_patch_importance_pooling = use_patch_importance_pooling\n",
    "        self.config.use_delta_transformer = use_delta_transformer\n",
    "        self.config.mm_hidden_size = vision_config.hidden_size\n",
    "        self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "        if not hasattr(self, 'pooling_layer') and use_patch_importance_pooling:\n",
    "            self.pooling_layer = nn.Linear(self.config.hidden_size * 256, 1)\n",
    "            self.patch_pooling_method = \"temporal_importance\"\n",
    "\n",
    "        if not hasattr(self, 'transformer_delta_encoder') and use_delta_transformer:\n",
    "            self.transforemr_adding_layer = nn.TransformerEncoderLayer(d_model=self.config.hidden_size, nhead=8, batch_first=True)\n",
    "            self.transformer_delta_encoder = nn.TransformerEncoder(self.transforemr_adding_layer, num_layers=1)\n",
    "            self.patch_pooling_method = \"temporal_transformer\"\n",
    "            self.position_matrix = torch.nn.Parameter(self.getPositionEncoding(seq_len=2048, d = self.config.hidden_size))\n",
    "            self.position_matrix.requires_grad = False\n",
    "\n",
    "        if not hasattr(self, 'mm_projector'):\n",
    "            self.mm_projector = nn.Linear(vision_config.hidden_size, self.config.hidden_size)\n",
    "\n",
    "        if pretrain_mm_mlp_adapter is not None:\n",
    "            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "            self.mm_projector.load_state_dict({k.split('.')[-1]: v for k, v in mm_projector_weights.items()})\n",
    "\n",
    "        return dict(\n",
    "            image_processor=image_processor,\n",
    "            image_token_len=num_patches,\n",
    "            vision_config=vision_config\n",
    "        )\n",
    "    def getPositionEncoding(self, seq_len=2048, d=5120, n=10000):\n",
    "        P = torch.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in torch.arange(int(d/2)):\n",
    "                denominator = torch.pow(n, 2*i/d)\n",
    "                P[k, 2*i] = torch.sin(k/denominator)\n",
    "                P[k, 2*i+1] = torch.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "    def text_importance_pooling(self,patch_feature):# 8, 256, 5120\n",
    "        # print(patch_feature.shape)\n",
    "        patch_feature_flatten = torch.flatten(patch_feature,start_dim=1)\n",
    "        score = nn.functional.softmax(self.pooling_layer(patch_feature_flatten), dim=0)\n",
    "        # print(score.shape)\n",
    "        score = score.unsqueeze(2)\n",
    "        patch_feature = score*patch_feature\n",
    "        patch_feature = torch.sum(patch_feature, dim=0)\n",
    "        return patch_feature\n",
    "\n",
    "    def temporal_tranforemr_delta_adding(self,patch_feature):# 8, 256, 5120\n",
    "        patch_feature = patch_feature.permute(1,0,2) # 256,8,5120\n",
    "        sequence_length = patch_feature.shape[1]\n",
    "        patch_number = patch_feature.shape[0]\n",
    "        position_embedding = self.position_matrix[:sequence_length,:].unsqueeze(0).type_as(patch_feature)# 1,8,5120\n",
    "        position_embedding = position_embedding.repeat(patch_number,1,1).to(patch_feature.device) # 256,8,5120\n",
    "        patch_feature_pos = patch_feature+position_embedding\n",
    "        patch_feature_delta = self.transformer_delta_encoder(patch_feature_pos)[:,-1,:]\n",
    "        patch_feature_mean = torch.mean(patch_feature, dim=1) # 256 , 4096\n",
    "        patch_feature = patch_feature_delta + patch_feature_mean\n",
    "        return patch_feature\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        images: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "\n",
    "        # HACK: replace back original embeddings for Valley pretraining\n",
    "        orig_embeds_params = getattr(self, 'orig_embeds_params', None)\n",
    "        # if orig_embeds_params is not None:\n",
    "        #     orig_embeds_params = orig_embeds_params[0]\n",
    "        #     with torch.no_grad():\n",
    "        #         self.get_input_embeddings().weight.data[:-2] = orig_embeds_params[:-2].data\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            print(torch.max(input_ids))\n",
    "            print(input_ids.shape)\n",
    "            print(torch.min(input_ids))\n",
    "            inputs_embeds = self.wte(input_ids)\n",
    "\n",
    "\n",
    "        vision_tower = getattr(self, 'vision_tower', None)\n",
    "        \n",
    "        if vision_tower is not None and (input_ids.shape[1] != 1 or self.training) and images is not None:\n",
    "            # TODO: this is a modified multimodal LLM -- Haotian Liu\n",
    "            # vision_tower = vision_tower[0]  # HACK: for FSDP\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                if type(images) is list:\n",
    "                    # variable length images\n",
    "                    image_features = []\n",
    "                    for image in images:\n",
    "                        image_forward_out = vision_tower(image, output_hidden_states=True)\n",
    "                        select_hidden_state_layer = getattr(self.config, \"mm_vision_select_layer\", -1)\n",
    "                        select_hidden_state = image_forward_out.hidden_states[select_hidden_state_layer]\n",
    "                        image_feature = select_hidden_state[:, :]\n",
    "                        image_features.append(image_feature)\n",
    "                else:\n",
    "                    image_features = []\n",
    "                    for batch_id in range(len(images)):\n",
    "                        image_forward_outs = vision_tower(images[batch_id], output_hidden_states=True)# 8,3,224,224\n",
    "                        select_hidden_state_layer = getattr(self.config, \"mm_vision_select_layer\", -1)\n",
    "                        select_hidden_state = image_forward_outs.hidden_states[select_hidden_state_layer]\n",
    "                        image_features.append(select_hidden_state[:, :])\n",
    "                    image_features = torch.stack(image_features)\n",
    "\n",
    "\n",
    "            if type(images) is list:\n",
    "                image_features = [self.mm_projector(image_feature) for image_feature in image_features]\n",
    "            else:\n",
    "                image_features = self.mm_projector(image_features)\n",
    "\n",
    "            dummy_image_features = torch.zeros(256, 1024, device=inputs_embeds.device, dtype=inputs_embeds.dtype)\n",
    "            dummy_image_features = self.mm_projector(dummy_image_features)\n",
    "\n",
    "            new_input_embeds = []\n",
    "            cur_image_idx = 0 # this index is for batch\n",
    "            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n",
    "                if (cur_input_ids == vision_tower.config.im_patch_token).sum() == 0:\n",
    "                    # multimodal LLM, but the current sample is not multimodal\n",
    "                    cur_input_embeds = cur_input_embeds + (0. * dummy_image_features).sum()\n",
    "                    new_input_embeds.append(cur_input_embeds)\n",
    "                    continue\n",
    "\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                # patch pooling method ( mean, max, text relative pooling )\n",
    "                if self.patch_pooling_method == 'mean':\n",
    "                    mean_image_features = torch.mean(cur_image_features[:,1:,:],dim=0) # 256 , 4096\n",
    "                elif self.patch_pooling_method == 'max':\n",
    "                    mean_image_features = torch.max(cur_image_features[:,1:,:],dim=0)[0] # 256 , 4096\n",
    "                elif self.patch_pooling_method == 'temporal_importance':\n",
    "                    mean_image_features = self.text_importance_pooling(cur_image_features[:,1:,:]) # 256 , 4096\n",
    "                elif self.patch_pooling_method == 'temporal_transformer':\n",
    "                    mean_image_features = self.temporal_tranforemr_delta_adding(cur_image_features[:,1:,:]) # 256 , 4096\n",
    "\n",
    "                frame_image_features = cur_image_features[:,0,:]# frame_length, 4096\n",
    "                num_patches = mean_image_features.shape[0]\n",
    "                # print(mean_image_features.shape)\n",
    "\n",
    "                if (cur_input_ids == vision_tower.config.im_start_token).sum() != (cur_input_ids == vision_tower.config.im_end_token).sum():\n",
    "                    raise ValueError(\"The number of im_start_token and im_end_token should be the same\")\n",
    "                image_start_tokens = torch.where(cur_input_ids == vision_tower.config.im_start_token)[0]\n",
    "                multi_iamge_index = 0 # this index is for multi_image\n",
    "                cur_new_input_embeds = cur_input_embeds.clone() # to save the new embed\n",
    "                for image_start_token_pos in image_start_tokens: #this loop is for multi_image in one piece\n",
    "                    cur_image_features = mean_image_features.to(device=cur_input_embeds.device)\n",
    "                    if cur_input_ids[image_start_token_pos + num_patches + 1] != vision_tower.config.im_end_token:\n",
    "                        raise ValueError(\"Seems that the image is cut.\")\n",
    "                    cur_new_input_embeds = torch.cat((cur_new_input_embeds[:image_start_token_pos+1], cur_image_features, cur_new_input_embeds[image_start_token_pos + num_patches + 1:]), dim=0)\n",
    "                    multi_iamge_index+=1\n",
    "\n",
    "                try:\n",
    "                    if (cur_input_ids == vision_tower.config.vi_start_token).sum() != (cur_input_ids == vision_tower.config.vi_end_token).sum():\n",
    "                        raise ValueError(\"The number of vi_start_token and vi_end_token should be the same\")\n",
    "                    video_start_tokens = torch.where(cur_input_ids == vision_tower.config.vi_start_token)[0]\n",
    "                    num_frame = frame_image_features.shape[0]\n",
    "                    assert (cur_input_ids == vision_tower.config.vi_frame_token).sum() == num_frame\n",
    "                    cur_video_input_embeds = cur_new_input_embeds.clone() # to save the new embed\n",
    "                    for video_start_token_pos in video_start_tokens: #this loop is for multi_image in one piece\n",
    "                        frame_image_features = frame_image_features.to(device=cur_input_embeds.device)\n",
    "                        if cur_input_ids[video_start_token_pos + num_frame + 1] != vision_tower.config.vi_end_token:\n",
    "                            raise ValueError(\"Seems that the image is cut.\")\n",
    "                        cur_video_input_embeds = torch.cat((cur_video_input_embeds[:video_start_token_pos+1], frame_image_features, cur_video_input_embeds[video_start_token_pos + num_frame + 1:]), dim=0)\n",
    "                except:\n",
    "                    cur_video_input_embeds = cur_new_input_embeds.clone()\n",
    "                new_input_embeds.append(cur_video_input_embeds)\n",
    "                cur_image_idx += 1\n",
    "            inputs_embeds = torch.stack(new_input_embeds, dim=0)\n",
    "\n",
    "        return super(ValleyGPTModel, self).forward(\n",
    "            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds, use_cache=use_cache,\n",
    "            output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "\n",
    "class ValleyGPTForCausalLM( MptForCausalLM):\n",
    "    config_class = ValleyConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = ValleyGPTModel(config)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        images: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            images=images\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model/pipeline parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"images\": kwargs.get(\"images\", None),\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    def initialize_vision_tokenizer(self, tokenizer):\n",
    "        vision_config = self.get_model().vision_tower.config\n",
    "        vision_config.use_im_start_end = True\n",
    "        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN,DEFAULT_VIDEO_FRAME_TOKEN], special_tokens=True)\n",
    "        self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, DEFAULT_VI_START_TOKEN,DEFAULT_VI_END_TOKEN], special_tokens=True)\n",
    "        self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n",
    "        vision_config.vi_start_token, vision_config.vi_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_VI_START_TOKEN, DEFAULT_VI_END_TOKEN])\n",
    "        vision_config.vi_frame_token = tokenizer.convert_tokens_to_ids(DEFAULT_VIDEO_FRAME_TOKEN)\n",
    "\n",
    "        if num_new_tokens > 0:\n",
    "            input_embeddings = self.get_input_embeddings().weight.data\n",
    "            output_embeddings = self.get_output_embeddings().weight.data\n",
    "\n",
    "            input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "                dim=0, keepdim=True)\n",
    "            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "                dim=0, keepdim=True)\n",
    "\n",
    "            input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "            output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "        vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n",
    "\n",
    "    def build_inputs(self,tokenizer, messages):\n",
    "        proMpt = ''\n",
    "        for m in messages:\n",
    "            if m['role'] == 'system':\n",
    "                proMpt += m['content'] +'\\n\\n' + '###'\n",
    "            elif m['role'] == 'user':\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * 256 + \\\n",
    "                DEFAULT_IM_END_TOKEN + DEFAULT_VI_START_TOKEN + \\\n",
    "                DEFAULT_VIDEO_FRAME_TOKEN * 8 + DEFAULT_VI_END_TOKEN\n",
    "                if '<video>'  in m['content'] or '<image>' in m['content']:\n",
    "                    message = m['content'].replace('<video>',replace_token)\n",
    "                    message = message.replace('<image>',replace_token)\n",
    "                    proMpt += ' ' + 'Human' + \": \" + message+' \\n' + '###'\n",
    "            elif m['role'] == 'assistent':\n",
    "                proMpt += ' ' + 'Assistent' + \": \" + m['content']+' \\n' + '###'\n",
    "            else:\n",
    "                raise ValueError(\"Role is only suport \\\"assistent\\\", \\\"human\\\" and \\\"system\\\".\")\n",
    "        if DEFAULT_IM_START_TOKEN not in proMpt:\n",
    "            raise ValueError(\"You need to specify the <video> token in the query\")\n",
    "        tokenizer.padding_side = 'left'\n",
    "        input_id = tokenizer([proMpt], padding=True)\n",
    "        return input_id\n",
    "\n",
    "    def process_response(self,outputs):\n",
    "        output = []\n",
    "        for i, out in enumerate(outputs):\n",
    "            while True:\n",
    "                cur_len = len(out)\n",
    "                out = out.strip()\n",
    "                for pattern in ['###', 'Assistant:', 'Response:', 'Valley:']:\n",
    "                    if out.startswith(pattern):\n",
    "                        out = out[len(pattern):].strip()\n",
    "                if len(out) == cur_len:\n",
    "                    break\n",
    "            try:\n",
    "                index = out.index('###')\n",
    "            except ValueError:\n",
    "                out += '###'\n",
    "                index = out.index(\"###\")\n",
    "            out = out[:index].strip()\n",
    "            output.append(out)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def completion(self, tokenizer, video: str, message: list, gen_kwargs:dict, device):\n",
    "        inputs = self.build_inputs(tokenizer, message)\n",
    "        input_ids = torch.as_tensor(inputs.input_ids).to(device)\n",
    "        images = load_video(video)\n",
    "        images = images.permute(1, 0, 2, 3)\n",
    "        images = images.unsqueeze(0).half().to(device)\n",
    "        stopping_criteria = KeywordsStoppingCriteria(['###'], tokenizer, input_ids)\n",
    "        output_ids = self.generate(input_ids = input_ids, images = images, stopping_criteria=[stopping_criteria],**gen_kwargs)\n",
    "        input_token_len = input_ids.shape[1]\n",
    "        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n",
    "        if n_diff_input_output > 0:\n",
    "            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\n",
    "        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)\n",
    "        response = self.process_response(outputs)\n",
    "        return response\n",
    "\n",
    "AutoConfig.register(\"valley\", ValleyConfig)\n",
    "AutoModelForCausalLM.register(ValleyConfig, ValleyGPTForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MlyLs51AUtB7"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import logging.handlers\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "import re\n",
    "server_error_msg = \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n",
    "moderation_msg = \"YOUR INPUT VIOLATES OUR CONTENT MODERATION GUIDELINES. PLEASE TRY AGAIN.\"\n",
    "\n",
    "handler = None\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch.distributed as dist\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    if 0 == 0:\n",
    "        trainable_params = [k for k,v in model.named_parameters() if v.requires_grad]\n",
    "        trainable_params_group = {}\n",
    "        for para in trainable_params:\n",
    "            layer_num = re.findall(r'layers.(\\d+)\\.',para)\n",
    "            if layer_num:\n",
    "                cur_layer = int(layer_num[0])\n",
    "                if para.replace('layers.'+layer_num[0],'layers.*') not in trainable_params_group:\n",
    "                    trainable_params_group[para.replace('layers.'+layer_num[0],'layers.*')] = layer_num[0]\n",
    "                elif cur_layer > int(trainable_params_group[para.replace('layers.'+layer_num[0],'layers.*')]):\n",
    "                    trainable_params_group[para.replace('layers.'+layer_num[0],'layers.*')] = layer_num[0]\n",
    "\n",
    "            else:\n",
    "                trainable_params_group[para] = '0'\n",
    "        table = PrettyTable(['Parameter Name','Max Layer Number'])\n",
    "        for key in trainable_params_group.keys():\n",
    "            table.add_row([key, str(int(trainable_params_group[key])+1)])\n",
    "\n",
    "        print(table)\n",
    "        total_num = sum([v.numel() for k,v in model.named_parameters()])\n",
    "        trainable_num = sum([v.numel() for k,v in model.named_parameters() if v.requires_grad])\n",
    "        print('Total: {:.2f}M'.format(total_num/1e6), ' Trainable: {:.2f}M'.format(trainable_num/1e6))\n",
    "\n",
    "def rank_zero_info(content: str, logger, print_type: str = \"info\"):\n",
    "    output_method = getattr(logger, print_type)\n",
    "    if 0 == 0:\n",
    "        output_method(content)\n",
    "\n",
    "\n",
    "def get_logger(name: str):\n",
    "    # logger initialize\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # handler\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    # formatter\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    # add handler\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def build_logger(logger_name, logger_filename):\n",
    "    global handler\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Set the format of root handlers\n",
    "    if not logging.getLogger().handlers:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "    logging.getLogger().handlers[0].setFormatter(formatter)\n",
    "\n",
    "    # Redirect stdout and stderr to loggers\n",
    "    stdout_logger = logging.getLogger(\"stdout\")\n",
    "    stdout_logger.setLevel(logging.INFO)\n",
    "    sl = StreamToLogger(stdout_logger, logging.INFO)\n",
    "    sys.stdout = sl\n",
    "\n",
    "    stderr_logger = logging.getLogger(\"stderr\")\n",
    "    stderr_logger.setLevel(logging.ERROR)\n",
    "    sl = StreamToLogger(stderr_logger, logging.ERROR)\n",
    "    sys.stderr = sl\n",
    "\n",
    "    # Get logger\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Add a file handler for all loggers\n",
    "    if handler is None:\n",
    "        os.makedirs(LOGDIR, exist_ok=True)\n",
    "        filename = os.path.join(LOGDIR, logger_filename)\n",
    "        handler = logging.handlers.TimedRotatingFileHandler(\n",
    "            filename, when='D', utc=True)\n",
    "        handler.setFormatter(formatter)\n",
    "\n",
    "        for name, item in logging.root.manager.loggerDict.items():\n",
    "            if isinstance(item, logging.Logger):\n",
    "                item.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "class StreamToLogger(object):\n",
    "    \"\"\"\n",
    "    Fake file-like stream object that redirects writes to a logger instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, logger, log_level=logging.INFO):\n",
    "        self.terminal = sys.stdout\n",
    "        self.logger = logger\n",
    "        self.log_level = log_level\n",
    "        self.linebuf = ''\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.terminal, attr)\n",
    "\n",
    "    def write(self, buf):\n",
    "        temp_linebuf = self.linebuf + buf\n",
    "        self.linebuf = ''\n",
    "        for line in temp_linebuf.splitlines(True):\n",
    "            # From the io.TextIOWrapper docs:\n",
    "            #   On output, if newline is None, any '\\n' characters written\n",
    "            #   are translated to the system default line separator.\n",
    "            # By default sys.stdout.write() expects '\\n' newlines and then\n",
    "            # translates them so this is still cross platform.\n",
    "            if line[-1] == '\\n':\n",
    "                self.logger.log(self.log_level, line.rstrip())\n",
    "            else:\n",
    "                self.linebuf += line\n",
    "\n",
    "    def flush(self):\n",
    "        if self.linebuf != '':\n",
    "            self.logger.log(self.log_level, self.linebuf.rstrip())\n",
    "        self.linebuf = ''\n",
    "\n",
    "\n",
    "def disable_torch_init():\n",
    "    \"\"\"\n",
    "    Disable the redundant torch default initialization to accelerate model creation.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n",
    "    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n",
    "\n",
    "\n",
    "def violates_moderation(text):\n",
    "    \"\"\"\n",
    "    Check whether the text violates OpenAI moderation API.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/moderations\"\n",
    "    headers = {\"Content-Type\": \"application/json\",\n",
    "               \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]}\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    data = \"{\" + '\"input\": ' + f'\"{text}\"' + \"}\"\n",
    "    data = data.encode(\"utf-8\")\n",
    "    try:\n",
    "        ret = requests.post(url, headers=headers, data=data, timeout=5)\n",
    "        flagged = ret.json()[\"results\"][0][\"flagged\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        flagged = False\n",
    "    except KeyError as e:\n",
    "        flagged = False\n",
    "\n",
    "    return flagged\n",
    "\n",
    "\n",
    "def pretty_print_semaphore(semaphore):\n",
    "    if semaphore is None:\n",
    "        return \"None\"\n",
    "    return f\"Semaphore(value={semaphore._value}, locked={semaphore.locked()})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196,
     "referenced_widgets": [
      "3cc4d38b0eb44c4fb63e10e285483fe8",
      "a631dd4449474540a4ae84670cafa360",
      "021fce3d64fd447e8fe757ad6a0e1b59",
      "f5a338b8aa88409eaa8d173a281878ca",
      "40a8149cf4ef4d5486026e163d202d80",
      "512a629fc24b4f1f95f9613695bf5596",
      "4f5a433f6df3489586ec7007632f0a5e",
      "a2e1f120a4c947d0b26b4e51dcfc428a",
      "f96716d5902642f8ad22984ab08fefc4",
      "e48ee0e347004d59a79fb7e22f6a2b50",
      "53cbc8b36580480d8f61af61314d7db9"
     ]
    },
    "id": "XqLFyxrbYqeB",
    "outputId": "36bb0c2b-f792-4268-cc44-2ed1d2e5f2dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bcb6c69a974b9686e846eead1a03a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "shard_0.zip:   0%|          | 0.00/6.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/content/video/shard_0.zip'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"Vividbot/vast2m_vi\", filename=\"video/shard_0.zip\", repo_type=\"dataset\", local_dir=\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yi4odjb7qfUP",
    "outputId": "87a9194a-b9ad-47f7-f402-e9cacceb2855",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip /content/video/shard_0.zip -d /content/video/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "J9Pt-G2HPfKT",
    "outputId": "f14f102e-b41f-44ae-e7e4-8a121b76e52b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "You are using a model of type mpt to instantiate a model of type valley. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = dict([('conf', 'valley_stage1.yaml')])\n",
    "train(args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17579"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "021fce3d64fd447e8fe757ad6a0e1b59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2e1f120a4c947d0b26b4e51dcfc428a",
      "max": 6836900804,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f96716d5902642f8ad22984ab08fefc4",
      "value": 6836900804
     }
    },
    "3cc4d38b0eb44c4fb63e10e285483fe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a631dd4449474540a4ae84670cafa360",
       "IPY_MODEL_021fce3d64fd447e8fe757ad6a0e1b59",
       "IPY_MODEL_f5a338b8aa88409eaa8d173a281878ca"
      ],
      "layout": "IPY_MODEL_40a8149cf4ef4d5486026e163d202d80"
     }
    },
    "40a8149cf4ef4d5486026e163d202d80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f5a433f6df3489586ec7007632f0a5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "512a629fc24b4f1f95f9613695bf5596": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53cbc8b36580480d8f61af61314d7db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2e1f120a4c947d0b26b4e51dcfc428a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a631dd4449474540a4ae84670cafa360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_512a629fc24b4f1f95f9613695bf5596",
      "placeholder": "​",
      "style": "IPY_MODEL_4f5a433f6df3489586ec7007632f0a5e",
      "value": "shard_0.zip: 100%"
     }
    },
    "e48ee0e347004d59a79fb7e22f6a2b50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5a338b8aa88409eaa8d173a281878ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e48ee0e347004d59a79fb7e22f6a2b50",
      "placeholder": "​",
      "style": "IPY_MODEL_53cbc8b36580480d8f61af61314d7db9",
      "value": " 6.84G/6.84G [00:59&lt;00:00, 113MB/s]"
     }
    },
    "f96716d5902642f8ad22984ab08fefc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
