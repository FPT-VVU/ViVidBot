{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dminhvu/miniconda3/envs/vividbot/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/dminhvu/miniconda3/envs/vividbot/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 52, in main\n",
      "    service.run()\n",
      "  File \"/home/dminhvu/miniconda3/envs/vividbot/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/home/dminhvu/miniconda3/envs/vividbot/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 111, in login\n",
      "    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)\n",
      "  File \"/home/dminhvu/miniconda3/envs/vividbot/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 307, in _login\n",
      "    raise ValueError(\"Invalid token passed!\")\n",
      "ValueError: Invalid token passed!\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c0b5f5990a4acb9b1af46d905a1602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767da77b123c4910a4124eeedcb45bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240791a876774480ba02835f5b1cba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/178M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3f3366f0bd4c64b88338d289ee859b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/178M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7613fd2ea149619f8e9eda306bdbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa7f0197bca4da6ad638cc51471a6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6899 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"5CD-AI/Viet-ComputerScience-VQA\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vividbot.data.processor.huggingface import HuggingFaceProcessor\n",
    "\n",
    "hf_processor = HuggingFaceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6899"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=700x975>,\n",
       " 'description': 'Bức ảnh hiển thị đoạn code Arduino. \\nĐoạn code được chia thành 3 phần: \\n- Phần đầu tiên giới thiệu vòng lặp for trong đó thực hiện các thao tác ClearData(), digitalWrite() và delay() \\n- Phần thứ 2 giới thiệu hàm DelayDisplay() dùng để hiển thị ma trận LED \\n- Phần cuối cùng giới thiệu hàm setup() dùng để khởi tạo các chân output cho LED ma trận.',\n",
       " 'conversations': [{'role': 'user',\n",
       "   'content': 'Đoạn code này được viết bằng ngôn ngữ nào?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Đoạn code được viết bằng ngôn ngữ lập trình Arduino, một ngôn ngữ dựa trên C++.'},\n",
       "  {'role': 'user', 'content': 'Hàm DelayDisplay() được dùng để làm gì?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Hàm DelayDisplay() được dùng để hiển thị ma trận LED theo một chu trình thời gian nhất định. Tham số truyền vào cho hàm là xtime, đại diện cho thời gian hiển thị của ma trận LED.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Vòng lặp for trong phần đầu tiên thực hiện những thao tác nào?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Vòng lặp for này thực hiện các thao tác ClearData(), digitalWrite() với chân CLK và chân Data, delay() sau mỗi lần thực hiện. Chân CLK và chân Data là hai chân điều khiển tín hiệu cho ma trận LED.  Chân CLK (Clock) được dùng để điều khiển tốc độ truyền tín hiệu, còn chân Data (Data) được dùng để truyền dữ liệu cho ma trận LED. Hàm ClearData() được dùng để xóa dữ liệu hiện tại trong bộ nhớ ma trận LED.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Trong hàm setup(), tại sao các chân A4, A5 được khai báo với chế độ OUTPUT?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Các chân A4 và A5 được khai báo với chế độ OUTPUT vì chúng được dùng để điều khiển các dòng dữ liệu cho ma trận LED. Chế độ OUTPUT cho phép Arduino truyền tín hiệu ra ngoài để điều khiển các thiết bị ngoại vi, chẳng hạn như ma trận LED.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Đoạn code này được sử dụng để thực hiện chức năng gì?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Đoạn code này được sử dụng để điều khiển một ma trận LED. Nó thực hiện chức năng hiển thị ma trận LED theo một chu trình thời gian nhất định. Hàm DelayDisplay() được sử dụng để tạo ra hiệu ứng nhấp nháy hoặc di chuyển cho ma trận LED.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 25841.32it/s]\n",
      "200it [00:00, 25774.62it/s]\n",
      "200it [00:00, 21334.74it/s]\n",
      "200it [00:00, 34365.46it/s]\n",
      "200it [00:00, 26106.71it/s]\n",
      "200it [00:00, 27944.33it/s]\n",
      "200it [00:00, 22849.77it/s]\n",
      "200it [00:00, 22499.82it/s]\n",
      "200it [00:00, 21488.31it/s]\n",
      "200it [00:00, 30940.57it/s]\n",
      "200it [00:00, 20701.37it/s]\n",
      "200it [00:00, 32089.85it/s]\n",
      "200it [00:00, 9392.37it/s]\n",
      "200it [00:00, 28930.22it/s]\n",
      "200it [00:00, 29712.07it/s]\n",
      "200it [00:00, 29521.76it/s]\n",
      "200it [00:00, 30301.29it/s]\n",
      "200it [00:00, 29012.27it/s]\n",
      "200it [00:00, 30484.08it/s]\n",
      "200it [00:00, 32962.43it/s]\n",
      "200it [00:00, 33139.53it/s]\n",
      "200it [00:00, 29052.46it/s]\n",
      "200it [00:00, 31732.96it/s]\n",
      "200it [00:00, 23552.26it/s]\n",
      "200it [00:00, 29837.83it/s]\n",
      "200it [00:00, 29436.81it/s]\n",
      "200it [00:00, 33672.96it/s]\n",
      "200it [00:00, 31610.99it/s]\n",
      "200it [00:00, 29574.84it/s]\n",
      "200it [00:00, 21572.86it/s]\n",
      "200it [00:00, 28529.77it/s]\n",
      "200it [00:00, 33327.80it/s]\n",
      "100it [00:00, 23017.80it/s]\n",
      "200it [00:00, 33682.43it/s]\n",
      "99it [00:00, 26001.01it/s]\n",
      "100it [00:00, 27465.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'image', 'description', 'conversations'],\n",
       "    num_rows: 6899\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_LABEL_QUESTIONS = [\n",
    "  \"Hình này nói về điều gì?\",\n",
    "  \"Bạn mô tả được gì từ hình ảnh này?\",\n",
    "  \"Hình ảnh này thể hiện điều gì?\",\n",
    "  \"Nội dung chính của bức ảnh này là gì?\",\n",
    "  \"Chủ đề chính của bức ảnh này là gì?\",\n",
    "  \"Bạn có thể giải thích ý nghĩa của hình ảnh này không?\",\n",
    "  \"Hình ảnh này đang truyền tải thông điệp gì?\",\n",
    "  \"Bạn có thể tóm tắt nội dung của hình ảnh này không?\",\n",
    "  \"Bạn có thể mô tả ngắn gọn về hình ảnh này không?\",\n",
    "  \"Hãy tóm tắt nội dung trong hình ảnh này.\",\n",
    "]\n",
    "\n",
    "\n",
    "hf_processor = HuggingFaceProcessor()\n",
    "\n",
    "\n",
    "def get_random_label_question(id: Optional[int]) -> str:\n",
    "  if id:\n",
    "    return _LABEL_QUESTIONS[id % len(_LABEL_QUESTIONS)]\n",
    "  else:\n",
    "    return np.random.choice(_LABEL_QUESTIONS)\n",
    "\n",
    "\n",
    "DATA_NAME = \"viet-compsci-vqa\"\n",
    "DATA_NAME_ALT = DATA_NAME.replace(\"-\", \"_\")\n",
    "\n",
    "os.makedirs(f\"{Path.home()}/data/{DATA_NAME}\", exist_ok=True)\n",
    "os.makedirs(f\"{Path.home()}/data/{DATA_NAME}/images\", exist_ok=True)\n",
    "\n",
    "\n",
    "def convert_message(message: dict):\n",
    "  role = message[\"role\"]\n",
    "  content = message[\"content\"]\n",
    "\n",
    "  return {\"from\": \"human\" if role == \"user\" else \"gpt\", \"value\": content}\n",
    "\n",
    "\n",
    "def process(batch: dict):\n",
    "  batch_ids = batch[\"id\"]\n",
    "  batch_images = batch[\"image\"]\n",
    "  batch_descriptions = batch[\"description\"]\n",
    "  batch_conversations = batch[\"conversations\"]\n",
    "\n",
    "  description_data = []\n",
    "  conversation_data = []\n",
    "\n",
    "  for i, (id, image, description, conversations) in tqdm(\n",
    "    enumerate(zip(batch_ids, batch_images, batch_descriptions, batch_conversations))\n",
    "  ):\n",
    "    if not image.format:\n",
    "      img_exts = [\"png\", \"jpg\"]\n",
    "    else:\n",
    "      img_exts = [image.format.lower()]\n",
    "\n",
    "    for img_ext in img_exts:\n",
    "      try:\n",
    "        # save image\n",
    "        if not os.path.exists(f\"{Path.home()}/data/{DATA_NAME}/images/{id}.{img_ext}\"):\n",
    "          image.save(f\"{Path.home()}/data/{DATA_NAME}/images/{id}.{img_ext}\")\n",
    "        conversations = [convert_message(m) for m in conversations][:10]\n",
    "        if \"<image>\" not in conversations[0][\"value\"]:\n",
    "          conversations[0][\"value\"] = f\"<image>\\n{conversations[0]['value']}\"\n",
    "\n",
    "        conversation_data.append(\n",
    "          {\n",
    "            \"id\": id,\n",
    "            \"image\": f\"images/{id}.{img_ext}\",\n",
    "            \"conversations\": conversations,\n",
    "            \"path\": f\"Vividbot/{DATA_NAME}/images\",\n",
    "          }\n",
    "        )\n",
    "        if i % int(round(len(ds) * 0.005)) == 0:\n",
    "          conversation_data.append(\n",
    "            {\n",
    "              \"id\": id,\n",
    "              \"image\": f\"images/{id}.{img_ext}\",\n",
    "              \"conversations\": [\n",
    "                {\n",
    "                  \"from\": \"human\",\n",
    "                  \"value\": f\"<image>\\n{get_random_label_question(id=i)}\"\n",
    "                  if i % 2 == 0\n",
    "                  else f\"{get_random_label_question(id=i)}\\n<image>\",\n",
    "                },\n",
    "                {\"from\": \"gpt\", \"value\": description},\n",
    "              ],\n",
    "              \"path\": f\"Vividbot/{DATA_NAME}/images\",\n",
    "            }\n",
    "          )\n",
    "\n",
    "        if len(description.split(\" \")) < 200:\n",
    "          description_data.append(\n",
    "            {\n",
    "              \"id\": id,\n",
    "              \"image\": f\"images/{id}.{img_ext}\",\n",
    "              \"conversations\": [\n",
    "                {\n",
    "                  \"from\": \"human\",\n",
    "                  \"value\": f\"<image>\\n{get_random_label_question(id=i)}\"\n",
    "                  if i % 2 == 0\n",
    "                  else f\"{get_random_label_question(id=i)}\\n<image>\",\n",
    "                },\n",
    "                {\"from\": \"gpt\", \"value\": description},\n",
    "              ],\n",
    "              \"path\": f\"Vividbot/{DATA_NAME}/images\",\n",
    "            }\n",
    "          )\n",
    "      except Exception:\n",
    "        pass\n",
    "\n",
    "  with open(f\"{Path.home()}/data/{DATA_NAME}/conversation.jsonl\", \"a\") as f:\n",
    "    for d in conversation_data:\n",
    "      f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "  with open(f\"{Path.home()}/data/{DATA_NAME}/description.jsonl\", \"a\") as f:\n",
    "    for d in description_data:\n",
    "      f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "ds.map(process, batched=True, batch_size=200, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_description_data = [\n",
    "  json.loads(line)\n",
    "  for line in open(f\"{Path.home()}/data/{DATA_NAME}/description.jsonl\", \"r\")\n",
    "]\n",
    "\n",
    "processed_conversation_data = [\n",
    "  json.loads(line)\n",
    "  for line in open(f\"{Path.home()}/data/{DATA_NAME}/conversation.jsonl\", \"r\")\n",
    "]\n",
    "\n",
    "pretrain_data = processed_description_data\n",
    "pretrain_data = sorted(pretrain_data, key=lambda x: x[\"id\"])\n",
    "\n",
    "finetune_data = processed_conversation_data\n",
    "finetune_data = sorted(finetune_data, key=lambda x: x[\"id\"])\n",
    "\n",
    "with open(f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}_pretrain.json\", \"w\") as f:\n",
    "  f.write(json.dumps(pretrain_data, ensure_ascii=False, indent=2))\n",
    "\n",
    "with open(f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}.json\", \"w\") as f:\n",
    "  f.write(json.dumps(finetune_data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4948, 7106)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrain_data), len(finetune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c827d1a858f40b88cfdfeebeb8da133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "viet_compsci_vqa.json:   0%|          | 0.00/22.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e5836ef20b469aa2b0899ac0c13758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conversation.jsonl:   0%|          | 0.00/20.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hf_processor.zip_and_upload_dir(\n",
    "#   dir_path=f\"{Path.home()}/data/{DATA_NAME}/images\",\n",
    "#   repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "#   path_in_repo=\"images/images.zip\",\n",
    "#   repo_type=\"dataset\",\n",
    "#   overwrite=True,\n",
    "# )\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}_pretrain.json\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=f\"{DATA_NAME_ALT}_pretrain.json\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}.json\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=f\"{DATA_NAME_ALT}.json\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/description.jsonl\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=\"description.jsonl\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/conversation.jsonl\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=\"conversation.jsonl\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1252\n",
      "1252\n"
     ]
    }
   ],
   "source": [
    "processed_conversation_data = open(\n",
    "  f\"{Path.home()}/data/{DATA_NAME}/conversation_{SAMPLE_COUNT}k.jsonl\"\n",
    ").readlines()\n",
    "processed_detail_data = open(\n",
    "  f\"{Path.home()}/data/{DATA_NAME}/detail_{SAMPLE_COUNT}k.jsonl\"\n",
    ").readlines()\n",
    "\n",
    "processed_conversation_data = [json.loads(d) for d in processed_conversation_data]\n",
    "processed_detail_data = [json.loads(d) for d in processed_detail_data]\n",
    "\n",
    "print(len(processed_conversation_data))\n",
    "print(len(processed_detail_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = processed_detail_data + processed_conversation_data\n",
    "\n",
    "combined_data = sorted(combined_data, key=lambda x: x[\"id\"])\n",
    "combined_data = [\n",
    "  {**d, \"path\": \"Vividbot/viet-handwriting-gemini-vqa/images\"} for d in combined_data\n",
    "]\n",
    "\n",
    "with open(\n",
    "  f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}_{SAMPLE_COUNT*2}k.jsonl\", \"w\"\n",
    ") as f:\n",
    "  for d in combined_data:\n",
    "    f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\n",
    "  f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}_{SAMPLE_COUNT*2}k_all.json\", \"w\"\n",
    ") as f:\n",
    "  f.write(json.dumps(combined_data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbdt2 = json.loads(\n",
    "  open(\n",
    "    f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}_{SAMPLE_COUNT*2}k_all.json\"\n",
    "  ).read()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'image': 'images/0.jpeg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nBài toán yêu cầu chứng minh điều gì ở phần a)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Phần a) của bài toán yêu cầu chứng minh rằng OPQ là tam giác cân. Lời giải cho phần này dựa vào việc chứng minh tứ giác MBQO là tứ giác nội tiếp, từ đó suy ra hai góc MQQ và MBO bằng nhau (cùng chắn cung MO). Tiếp theo, tương tự với tứ giác MPCO,  ta cũng có hai góc MPO và MCO bằng nhau.  Từ đó suy ra góc MPO bằng góc MQO,  hay tam giác OPQ cân tại O.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Phần b) của bài toán giải quyết vấn đề gì?  Nêu rõ các bước chứng minh.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Phần b) của bài toán yêu cầu chứng minh rằng CR bằng HM.  Để chứng minh điều này,  bài toán sử dụng các tam giác đồng dạng.  Cụ thể,  ta có tam giác APR đồng dạng với tam giác ACOH.  Từ đó suy ra tỉ lệ:  PC/RC = CO/OH.  Tiếp theo,  ta có tam giác AMOH đồng dạng với tam giác APOC,  từ đó suy ra tỉ lệ:  MH/HO = PC/OC.  Kết hợp hai tỉ lệ trên,  ta suy ra CR bằng MH.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Phần c) của bài toán muốn chứng minh điều gì?  Cách giải quyết như thế nào?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Phần c) của bài toán yêu cầu chứng minh rằng tứ giác ACDK là tứ giác nội tiếp.  Để chứng minh điều này,  bài toán sử dụng các tam giác đồng dạng.  Cụ thể,  ta có tam giác AHB đồng dạng với tam giác ACO,  từ đó suy ra tỉ lệ:  AB/BH = AO/CO.  Tiếp theo,  ta có tam giác ABK đồng dạng với tam giác AOD,  từ đó suy ra tỉ lệ:  AB/BK = AO/OD.  Kết hợp hai tỉ lệ trên,  ta suy ra góc AKB bằng góc ADO.  Từ đó,  ta kết luận rằng tứ giác ACDK là tứ giác nội tiếp.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Phần d) của bài toán giải quyết vấn đề gì?  Nêu rõ các bước chứng minh.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Phần d) của bài toán yêu cầu chứng minh rằng ABEC là hình thoi.  Để chứng minh điều này,  bài toán sử dụng tính chất của điểm đối xứng.  Cụ thể,  do K là điểm đối xứng với H qua B,  nên KB bằng KH.  Tiếp theo,  do tứ giác ACDK là tứ giác nội tiếp,  nên góc KAB bằng góc KEB.  Kết hợp hai điều trên,  ta suy ra ABEC là hình thoi.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Hình vẽ minh họa trong bài toán có thể được sử dụng để chứng minh các kết quả nào khác?  Hãy đưa ra một ví dụ.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Ngoài các kết quả đã được chứng minh trong bài toán,  hình vẽ minh họa có thể được sử dụng để chứng minh các kết quả khác.  Ví dụ,  ta có thể chứng minh rằng tứ giác AMBN là tứ giác nội tiếp.  Để chứng minh điều này,  ta có thể sử dụng các góc nội tiếp và các góc ở tâm.  Cụ thể,  ta có:  góc AMB bằng 1/2 góc AOB (góc nội tiếp chắn cung AB).  Góc ANB bằng 1/2 góc AOB (góc ở tâm chắn cung AB).  Từ đó suy ra góc AMB bằng góc ANB,  hay tứ giác AMBN là tứ giác nội tiếp.'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbdt2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6442284fc24949ea9b88e7a73064388f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "images.zip:   0%|          | 0.00/220M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_processor.zip_and_upload_dir(\n",
    "  dir_path=f\"{Path.home()}/data/{DATA_NAME}/images\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=\"images/images.zip\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/{DATA_NAME_ALT}_{SAMPLE_COUNT*2}k.jsonl\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=f\"{DATA_NAME_ALT}_{SAMPLE_COUNT*2}k.jsonl\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/viet_handwriting_vqa.json\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=\"viet_handwriting_vqa.json\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/conversation_{SAMPLE_COUNT}k.jsonl\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=f\"conversation_{SAMPLE_COUNT}k.jsonl\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")\n",
    "\n",
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/detail_{SAMPLE_COUNT}k.jsonl\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=f\"detail_{SAMPLE_COUNT}k.jsonl\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_processor.upload_file(\n",
    "  file_path=f\"{Path.home()}/data/{DATA_NAME}/detail_{SAMPLE_COUNT}k.jsonl\",\n",
    "  repo_id=f\"Vividbot/{DATA_NAME}\",\n",
    "  path_in_repo=f\"detail_{SAMPLE_COUNT}k.jsonl\",\n",
    "  repo_type=\"dataset\",\n",
    "  overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = json.load(open(f\"{Path.home()}/data/viet_handwriting_vqa.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{**d, \"path\": \"Vividbot/viet-handwriting-vqa/images\"} for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{Path.home()}/data/viet_handwriting_vqa.json\", \"w\") as f:\n",
    "  f.write(json.dumps(data, ensure_ascii=False, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vividbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
