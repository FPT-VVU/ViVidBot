{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count (x):\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"test.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    for i in data:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=[\"/home/duytran/Downloads/vividbot_data/Vast2M.json\"])\n",
    "print(data[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "# translate text\n",
    "text = \"a man is using a spoon to scoop something out of a pan that is sitting on top of a pan.\"\n",
    "dest_text = translator.translate(text, dest='vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from yt_dlp.utils import download_range_func\n",
    "start =302\n",
    "end = 400\n",
    "yt_opts = { 'format': 'best[ext=mp4]', \n",
    "           'download_ranges': download_range_func(None, [(start, end)]), \n",
    "           'force_keyframes_at_cuts': True, \n",
    "           \"outtmpl\": \"/home/duytran/Downloads/a32f1Dyd2GM.%(ext)s\"}\n",
    "url = \"https://www.youtube.com/watch?v=a32f1Dyd2GM\"\n",
    "with yt_dlp.YoutubeDL(yt_opts) as ydl:\n",
    "  ydl.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"json\", data_files=\"/home/duytran/Desktop/ViVidBot/Vast2M_vi.json\")\n",
    "for i in data[\"train\"]:\n",
    "    print(i[\"conversation\"][0][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ['a man is using a spoon to scoop something out of a pan that is sitting on top of a pan.', 'a chef pours ingredients on a tray and then adds some green food.', 'a person is using a green bottle on something and is stirring it.', 'an individual pours a sauce in his hand onto some type of food', 'a person is pouring food into a tray of pans and then mixing it']\n",
    "dest_temp = translator.translate(temp, dest='vi')\n",
    "print([i.text for i in dest_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data[\"train\"]:\n",
    "    print(translator.translate(item[\"vision_cap\"], dest='vi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, groq_api_key=\"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\", model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"dịch \\{a man is using a spoon to scoop something out of a pan that is sitting on top of a pan.\\}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"vinai/PhoGPT-4B-Chat\"  \n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)  \n",
    "config.init_device = \"cuda\"\n",
    "#config.attn_config['attn_impl'] = 'flash' # If installed: this will use either Flash Attention V1 or V2 depending on what is installed\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "# If your GPU does not support bfloat16:\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model.eval()  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  \n",
    "\n",
    "PROMPT_TEMPLATE = \"### Câu hỏi: {instruction}\\n### Trả lời:\"  \n",
    "\n",
    "# Some instruction examples\n",
    "# instruction = \"Viết bài văn nghị luận xã hội về {topic}\"\n",
    "# instruction = \"Viết bản mô tả công việc cho vị trí {job_title}\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\n{sentence_or_paragraph}\"\n",
    "# instruction = \"Dựa vào văn bản sau đây:\\n{text}\\nHãy trả lời câu hỏi: {question}\"\n",
    "# instruction = \"Tóm tắt văn bản:\\n{text}\"\n",
    "\n",
    "instruction = \"dịch \\\"A man is talking through a field of wildflowers, talking to the cameraman, and advising to always get permission before harvesting on any land.\\\" sang tiếng Việt\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\nTriệt phá băng nhóm kướp ô tô, sử dụng \\\"vũ khí nóng\\\"\"\n",
    "\n",
    "input_prompt = PROMPT_TEMPLATE.format_map({\"instruction\": instruction})  \n",
    "\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n",
    "\n",
    "outputs = model.generate(  \n",
    "    inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "    attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "    do_sample=True,  \n",
    "    temperature=1.0,  \n",
    "    top_k=50,  \n",
    "    top_p=0.9,  \n",
    "    max_new_tokens=1024,  \n",
    "    eos_token_id=tokenizer.eos_token_id,  \n",
    "    pad_token_id=tokenizer.pad_token_id  \n",
    ")  \n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "response = response.split(\"### Trả lời:\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"dịch \\\"A man is talking through a field of wildflowers, talking to the cameraman, and advising to always get permission before harvesting on any land.\\\" sang tiếng Việt\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\nTriệt phá băng nhóm kướp ô tô, sử dụng \\\"vũ khí nóng\\\"\"\n",
    "\n",
    "input_prompt = PROMPT_TEMPLATE.format_map({\"instruction\": instruction})  \n",
    "\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n",
    "\n",
    "outputs = model.generate(  \n",
    "    inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "    attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "    do_sample=True,  \n",
    "    temperature=1.0,  \n",
    "    top_k=50,  \n",
    "    top_p=0.9,  \n",
    "    max_new_tokens=1024,  \n",
    "    eos_token_id=tokenizer.eos_token_id,  \n",
    "    pad_token_id=tokenizer.pad_token_id  \n",
    ")  \n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "response = response.split(\"### Trả lời:\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=[\"/home/duytran/Downloads/vast27m_annotations/annotations.json\"], streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(data['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"liuhaotian/LLaVA-Pretrain\", streaming=True)\n",
    "print(next(iter(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "# hugging face authen key\n",
    "login(token=\"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\")\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/home/duytran/Downloads/vast27m_annotations/annotations.json\",\n",
    "    path_in_repo=\"vast27_enannotations.json\",\n",
    "    repo_id=\"Vividbot/vast27_en\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"Vividbot/vast27_en\", streaming=True)\n",
    "print(next(iter(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=[\"/home/duytran/Desktop/ViVidBot/data/result.json\"], streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data[\"train\"]:\n",
    "    print(i[\"vast_cap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "\n",
    "path = \"/home/duytran/Downloads/vast27m_annotations/annotations.json\"\n",
    "\n",
    "# save first 1 million items\n",
    "result = open(\"result.json\", \"w\")\n",
    "with open(path) as f:\n",
    "    for i, item in enumerate(ijson.items(f, \"item\")):\n",
    "        if i > 1000000:\n",
    "            break\n",
    "        result.write(json.dumps(item) + \"\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "path = \"/home/duytran/Desktop/ViVidBot/data/chat.json\"\n",
    "\n",
    "data = load_dataset(\"json\", data_files=[path])\n",
    "\n",
    "print(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data[\"train\"]:\n",
    "    print(i[\"conversations\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_collect = []\n",
    "\n",
    "for item in data[\"train\"]:\n",
    "    if item[\"conversations\"][0][\"value\"] not in human_collect:\n",
    "        human_collect.append(item[\"conversations\"][0][\"value\"])\n",
    "\n",
    "print(len(human_collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in human_collect:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to txt\n",
    "with open(\"human.txt\", \"w\") as f:\n",
    "    for item in human_collect:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDexs4YZqNvy6ij_qryTMaz3DyybH0pFKw\")\n",
    "model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=[\"/home/duytran/Desktop/ViVidBot/data/Vast2M.json\"])\n",
    "\n",
    "print(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data[\"train\"].select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test = open(\"temp.json\", \"w\")\n",
    "result_all = []\n",
    "for i in range(1000):\n",
    "    result = {\"new_vast\": \"\"}\n",
    "    input = data[\"train\"][i][\"vast_cap\"]\n",
    "    response = model.generate_content(f\"\"\"translate \\\"{input}\\\" from English to Vietnamese with natural tone and without grammar incorrect, just return as format \n",
    "   <a>content translation<a>\"\"\")\n",
    "    result[\"new_vast\"] = response.text\n",
    "    print(result)\n",
    "    result_all.append(result)\n",
    "    break\n",
    "test.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(batch):\n",
    "    result = []\n",
    "    for item in batch[\"vast_cap\"]:\n",
    "        response = model.generate_content(f\"translate \\\"{item}\\\" from English to Vietnamese with natural tone and without grammar incorrect, just return the result of translation\")\n",
    "        result.append(response.text)\n",
    "    batch[\"vast_cap\"] = result\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.map(map_fn, batched=True, batch_size=100, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=OXi4T58PwdM\"\n",
    "\n",
    "# times in seconds\n",
    "start = \"00:10:00\"\n",
    "end = \"00:15:00\"\n",
    "\n",
    "ffmpeg_args = {\n",
    "  # - Don't forget the _i after \"ffmpeg\"; this puts the arguments before ffmpeg's `-i` argument,\n",
    "  #   thus short-circuiting the download itself. Fail to do that,\n",
    "  #   and you might as well skip ffmpeg for the download and trim in post-processing.\n",
    "  # - Note that the arguments are pre-parsed into a list, like you'd pass to `subprocess.run`.\n",
    "  \"ffmpeg_i\": [\"-ss\", str(start), \"-to\", str(end)]  \n",
    "}\n",
    "\n",
    "opts = {\n",
    "  \"external_downloader\": \"ffmpeg\",\n",
    "  \"external_downloader_args\": ffmpeg_args,\n",
    "  # though not required, I'm including the subtitles options here for a reason; see below\n",
    "  \"writesubtitles\": False,\n",
    "  \"writeautomaticsub\": False,\n",
    "  # to suppress ffmpeg's stdout output\n",
    "  \"quiet\": True,\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(opts) as ydl:\n",
    "  ydl.download(url)\n",
    "  \n",
    "  # If you want WebVTT captions, yt-dlp will fail to download them if you're using ffmpeg.\n",
    "  # This isn't ffmpeg's fault; it's because yt-dlp (as of this writing) forces ffmpeg to use\n",
    "  # the stream copy encoder (look for `args += ['-c', 'copy']` in downloader/external.py).\n",
    "  # yt-dlp hosts their own builds of ffmpeg, and one of them supposedly fixes this problem\n",
    "  # by ignoring certain WebVTT header lines, but why would you want to install a custom build\n",
    "  # to download a less informative version of the caption files?\n",
    "  # Anyway, we can't get around this with any other options that I've found, \n",
    "  # so we'll run a second download to get captions.\n",
    "  \n",
    "  # Note that you can create a new YouTubeDL instance with a new options dictionary, but the\n",
    "  # constructor is a bit expensive, so I'm including an example of reusing a built instance\n",
    "  # for kicks. This dictionary tweaking is likely best separated out into its own function.\n",
    "  opts = {\n",
    "    **ydl.params,\n",
    "    \"external_downloader\": \"native\",\n",
    "    \"external_downloader_args\": {},\n",
    "    \"writesubtitles\": True,\n",
    "    # if you also want automatically generated captions/subtitles\n",
    "    \"writeautomaticsub\": True,\n",
    "    # so we only get the captions and don't download the (whole) video again\n",
    "    \"skip_download\": True,\n",
    "  }\n",
    "  ydl.params = opts\n",
    "  ydl.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duytran/miniconda3/envs/vividbot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000001\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"json\", data_files=\"/home/duytran/Downloads/vividbot_data/Vast2M.json\")\n",
    "\n",
    "print(len(data[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 62501/62501 [00:00<00:00, 970807.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1031007.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1055844.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1049692.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1079959.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1110991.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1112306.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1099814.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1099555.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1089289.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1081041.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1092694.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1094556.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1087978.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1074351.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1066163.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 887766.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1114965.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1096272.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1073872.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1083732.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1075824.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1088913.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1090871.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1094757.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1039828.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1059248.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1092977.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1091202.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1077597.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1081411.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 62500/62500 [00:00<00:00, 1078174.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# devide dataset into num chunks\n",
    "num_shards = 32\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = data[\"train\"].shard(num_shards=num_shards, index=shard_idx, contiguous=True)\n",
    "    shard.save_to_disk(f\"/home/duytran/Downloads/cache_dir/shard_{shard_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "shard_0 = load_from_disk(f\"/home/duytran/Downloads/cache_dir/shard_31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload later\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "ds = concatenate_datasets([\n",
    "    load_from_disk(f\"/home/duytran/Downloads/cache_dir/shard_{shard_idx}\")\n",
    "    for shard_idx in range(num_shards)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000001\n"
     ]
    }
   ],
   "source": [
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# read number folder in a folder\n",
    "import os\n",
    "path = \"/home/duytran/Downloads/cache_dir\"\n",
    "\n",
    "print(len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 39] Directory not empty: '/home/duytran/Downloads/cache_dir/temp/shard_0/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/duytran/Downloads/cache_dir/temp/shard_0/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 39] Directory not empty: '/home/duytran/Downloads/cache_dir/temp/shard_0/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.rmdir(\"/home/duytran/Downloads/cache_dir/temp/shard_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the folder you want to remove\n",
    "folder_path = '/home/duytran/Downloads/cache_dir/temp/shard_0'\n",
    "\n",
    "# Remove the folder and all its contents\n",
    "shutil.rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vividbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
