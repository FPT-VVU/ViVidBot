{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "l = [ \"People wearing harnesses using ropes to climb up a rock slope.\", \"A person climbing down a rock edge while someone talks about donating to a cause.\", \"A person is repelling down a rock while a girl asks for donations.\", \"A woman is describing some fears you may have while rock climbing.\", \"A woman is shown rock climbing and coming down from the rock, she then hugs a man when she is down.\", \"a man going down a rock cliff using a harness and rope with another person nearby\", \"A woman is narrating in the background as mountain climbers scale the side of a mountain.\", \"A woman is talking as people are using ropes lo lower themselves down a mountain.\", \"A group of people watch as woman scales down the side of a mountain.\", \"A woman climbs on a rock formation using safety ropes to protect her.\" ]\n",
    "\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "TRANSLATED_METADATA_PATH = f\"{Path.home()}/data/vast-2m/metadata\"\n",
    "ORIGINAL_METADATA_PATH = f\"{Path.home()}/data/vast-2m/vast_2m_chunk_e\"\n",
    "TEMP_REFINED_METADATA_PATH = f\"{Path.home()}/data/vast-2m/metadata-refined-temp\"\n",
    "REFINED_METADATA_PATH = f\"{Path.home()}/data/vast-2m/metadata-refined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"json\", data_files=f\"{Path.home()}/data/vast-2m/metadata-refined-temp/translated-metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 5756.88it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(TEMP_REFINED_METADATA_PATH, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(500)):\n",
    "  if i == 301:\n",
    "    original_metadata_file = f\"{ORIGINAL_METADATA_PATH}/shard_{i}.json\"\n",
    "\n",
    "    for data in open(original_metadata_file).readlines():\n",
    "      data = json.loads(data)\n",
    "      \n",
    "      metadata = {\n",
    "        \"id\": data[\"clip_id\"],\n",
    "        \"video\": f\"shard_{i}/{data['clip_id']}.mp4\",\n",
    "        \"vision_cap\": data[\"vision_cap\"]\n",
    "      }\n",
    "\n",
    "      refined_metadata_file = f\"{TEMP_REFINED_METADATA_PATH}/shard_{i}.jsonl\"\n",
    "      with open(refined_metadata_file, \"a\") as f:\n",
    "        f.write(json.dumps(metadata, separators=(',', ':')) + \"\\n\")\n",
    "\n",
    "    continue\n",
    "\n",
    "  # if i != 301:\n",
    "  #   continue\n",
    "  \n",
    "  translated_metadata_file = f\"{TRANSLATED_METADATA_PATH}/shard_{i}.json\"\n",
    "  original_metadata_file = f\"{ORIGINAL_METADATA_PATH}/shard_{i}.json\"\n",
    "\n",
    "  translated_metadatas = [json.loads(data) for data in open(translated_metadata_file).readlines()]\n",
    "  translated_ids = [data[\"id\"] for data in translated_metadatas]\n",
    "  original_metadatas = [json.loads(data) for data in open(original_metadata_file).readlines()]\n",
    "  original_metadatas = [data for data in original_metadatas if \"clip_id\" in data and data[\"clip_id\"] in translated_ids ]\n",
    "\n",
    "  # sort by id\n",
    "  translated_metadatas.sort(key=lambda x: x[\"id\"])\n",
    "  original_metadatas.sort(key=lambda x: x[\"clip_id\"])\n",
    "\n",
    "  assert len(translated_metadatas) == len(original_metadatas), f\"{len(translated_metadatas)} != {len(original_metadatas)}\"\n",
    "\n",
    "  # print(translated_metadatas[:10])\n",
    "  # print(original_metadatas[:10])\n",
    "\n",
    "  # break\n",
    "\n",
    "  refined_data = []\n",
    "\n",
    "  for translated_metadata, original_metadata in tqdm(zip(translated_metadatas, original_metadatas)):\n",
    "    \n",
    "    id = translated_metadata[\"id\"]\n",
    "    clip_id = original_metadata[\"clip_id\"]\n",
    "\n",
    "    assert id == clip_id, f\"{id} != {clip_id}\"\n",
    "    video = translated_metadata[\"video\"]\n",
    "    vision_cap = original_metadata[\"vision_cap\"]\n",
    "\n",
    "    refined_data.append({\n",
    "      \"id\": id,\n",
    "      \"video\": video,\n",
    "      \"vision_cap\": vision_cap\n",
    "    })\n",
    "\n",
    "  # print(refined_data)\n",
    "\n",
    "  refined_metadata_file = f\"{TEMP_REFINED_METADATA_PATH}/shard_{i}.jsonl\"\n",
    "  with open(refined_metadata_file, \"w\") as f:\n",
    "    for data in refined_data:\n",
    "      f.write(json.dumps(data, separators=(',', ':')) + \"\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_199: 1240 != 3883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(500):\n",
    "  \n",
    "\n",
    "  translated_metadata_file = f\"{REFINED_METADATA_PATH}/shard_{i}.jsonl\"\n",
    "  original_metadata_file = f\"{TEMP_REFINED_METADATA_PATH}/shard_{i}.jsonl\"\n",
    "\n",
    "  if not os.path.exists(translated_metadata_file):\n",
    "    continue\n",
    "\n",
    "  translated_metadatas = [json.loads(data) for data in open(translated_metadata_file).readlines()]\n",
    "\n",
    "  original_metadatas = [json.loads(data) for data in open(original_metadata_file).readlines()]\n",
    "\n",
    "\n",
    "  if len(translated_metadatas) != len(original_metadatas):\n",
    "    print(f\"shard_{i}: {len(translated_metadatas)} != {len(original_metadatas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922226\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "s = 0\n",
    "\n",
    "for i in range(500):\n",
    "  \n",
    "\n",
    "  translated_metadata_file = f\"{REFINED_METADATA_PATH}/shard_{i}.jsonl\"\n",
    "  original_metadata_file = f\"{TEMP_REFINED_METADATA_PATH}/shard_{i}.jsonl\"\n",
    "\n",
    "  if not os.path.exists(translated_metadata_file):\n",
    "    continue\n",
    "\n",
    "  translated_metadatas = [json.loads(data) for data in open(translated_metadata_file).readlines()]\n",
    "\n",
    "  s += len(translated_metadatas)\n",
    "\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vividbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
